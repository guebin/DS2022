{
  
    
        "post0": {
            "title": "lecture",
            "content": "&#54217;&#44032;&#51648;&#54364; . &#45796;&#50577;&#54620; &#54217;&#44032;&#51648;&#54364;&#46308; . - 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?) . - 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values . 이걸 다 암기하는건 불가능함. | 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자! | . confusion matrix&#51032; &#51060;&#54644; . - 표1 . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP | FN | . 안나감(실제) | FP | TN | . - 표2 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | $(y, hat{y})= $ (O,O) | $(y, hat{y})= $(O,X) | . 안나감(실제) | $(y, hat{y})= $(X,O) | $(y, hat{y})= $(X,X) | . - 표3 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ # O/O$ | FN, $ #O/X$ | . 안나감(실제) | FP, $ #X/O$ | TN, $ #X/X$ | . 암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것 | . - 표4 (위키등에 있음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ text{#}O/O$ | FN, $ text{#}O/X$ | Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN$}$=$ frac{ mbox{#}O/O}{ textbf{#}O/O+ text{#}O/X}$ | . 안나감(실제) | FP, $ text{#}X/O$ | TN, $ text{#}X/X$ | | . | Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ text{#}O/O}{ text{#}O/O+ text{#}X/O}$ | | Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ text{#}O/O+ text{#}X/X}{total}$ | . &#49345;&#54889;&#44537; . - 최규빈은 입사하여 &quot;퇴사자 예측시스템&quot;의 개발에 들어갔다. . - 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다. . Accuracy . - 정의: Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ textbf{#}O/O+ textbf{#}X/X}{total}$ . 한국말로는 정확도, 정분류율이라고 한다. | 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음) | . - (상확극 시점1) 왜 애큐러시는 불충분한가? . 회사: 퇴사자예측프로그램 개발해 | 최규빈: 귀찮은데 다 나간다고 하자! -&gt; 99퍼의 accuracy | . 모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다? . Sensitivity(&#48124;&#44048;&#46020;), Recall(&#51116;&#54788;&#50984;), True Positive Rate(TPR) . - 정의: Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN$}$=$ frac{ text{#}O/O}{ text{#}O/O+ text{#}O/X}$ . 분모: 실제 O인 관측치 수 | 분자: 실제 O를 O라고 예측한 관측치 수 | 뜻: 실제 O를 O라고 예측한 비율 | . - (상황극 시점2) recall을 봐야하는 이유 . 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) | 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요? | . 인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다! 예시1:실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50% &gt; 예시2:최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0% . | . 결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다) | . Precision . - 정의: Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ text{#}O/O}{ text{#}O/O+ text{#}X/O}$ . 분모: O라고 예측한 관측치 | 분자: O라고 예측한 관측치중 진짜 O인 관측치 | 뜻: O라고 예측한 관측치중 진짜 O인 비율 | . - (상황극 시점3) recall 만으로 불충분한 이유 . 최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감! | . 이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로). . 인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요. . | 인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요. . | . 최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요? | . 인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 $ frac{10}{100}$이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!) | . F1 score . - 정의: recall과 precision의 조화평균 . - (상황극 시점4) recall, precision을 모두 고려 . 최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다. . | 최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까? . | . 인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요. | . Sensitivity(&#53945;&#51060;&#46020;), False Positive Rate(FPR) . - 정의: . (1) Sensitivity(특이도)=$ frac{TN}{FP+TN}$=$ frac{ text{#}X/X}{ text{#}X/O+ text{#}X/X}$ . (2) False Positive Rate (FPR) = 1-Sensitivity(특이도) = $ frac{FP}{FP+TN}$=$ frac{ text{#}X/O}{ text{#}X/O+ text{#}X/X}$ . - 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ . sensitivity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. | FPR은 recall을 올리기 위해서 &quot;실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들&quot; 의 비율이다. | . 즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율.. . ROC curve . - 정의: $x$축=FPR, $y$축=TPR 을 그린 커브 . - 의미: . 결국 &quot;오해해서 미안해 vs recall&quot;을 그린 곡선이 ROC커브이다. | 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. | 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다. | .",
            "url": "https://guebin.github.io/DS2022/2022/05/08/Untitled.html",
            "relUrl": "/2022/05/08/Untitled.html",
            "date": " • May 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(10주차) 5월4일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tf.config.experimental.list_physical_devices() . 2022-05-06 10:37:51.868820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (1): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; sigmoid &#49440;&#53469; . - 기본버전은 아래와 같다 . $$y_i approx text{sigmoid}(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})= frac{ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}{1+ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}$$ . - 벡터버전은 아래와 같다. . $${ boldsymbol y} approx text{sigmoid}({ bf X}{ bf W} + b) = frac{ exp({ bf XW} +b)}{1+ exp({ bf XW} +b)}$$ . - 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}$: (n,1) matrix . | ${ bf W}$: (784,1) matrix . | $b$: (1,1) matrix . | +, exp 는 브로드캐스팅 . | . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (2): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; softmax &#49440;&#53469; . - $y_i=0 text{ or } 1$ 대신에 $ boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] text { or } [0,1]$와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?) . - 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다. . $$[{ boldsymbol y}_1 ~ { boldsymbol y}_2] propto [ { bf X}{ bf W}_1 ~ { bf X}{ bf W}_2] + [b_1 ~ b_2]= { bf X} [{ bf W}_1 { bf W}_2] + [b_1 ~ b_2]= { bf X}{ bf W} + { boldsymbol b}$$ . 여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}_1,{ boldsymbol y}_2$: (n,1) matrix . | ${ boldsymbol y}:=[{ boldsymbol y}_1~ { boldsymbol y}_2]$: (n,2) matrix . | ${ bf W}_1$, ${ bf W}_2$: (784,1) matrix . | ${ bf W}:=[{ bf W}_1~ { bf W}_2]$: (784,2) matrix . | $b_1,b_2$: (1,1) matrix . | $ boldsymbol{b}:= [b_1 ~b_2] $: (1,2) matrix . | + 는 브로드캐스팅 . | . - 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 ${ bf X} { bf W}_1 + b_1$와 ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 ${ boldsymbol y}_1$와 ${ boldsymbol y}_2$의 row값이 1이어야 함 . ${ boldsymbol y}_1 propto { bf X} { bf W}_1 + b_1$ $ to$ ${ bf X} { bf W}_1 + b_1$의 row값이 클수록 $ boldsymbol{y}_1$의 row 값이 1이라면 모형계수를 잘 추정한것 | ${ boldsymbol y}_2 propto { bf X} { bf W}_2 + b_2$ $ to$ ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 $ boldsymbol{y}_2$의 row 값을 1이라면 모형계수를 잘 추정한것 | . - (문제) ${ bf X}{ bf W}_1 +b_1$의 값이 500, ${ bf X}{ bf W}_2 +b_2$의 값이 200 인 row가 있다고 하자. 대응하는 $ boldsymbol{y}_1, boldsymbol{y}_2$의 row값은 얼마로 적합되어야 하는가? . (1) $[0,0]$ . (2) $[0,1]$ . (3) $[1,0]$ &lt;-- 이게 답이다! . (4) $[1,1]$ . . Note: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. ${ bf X}{ bf W}_1 +b_1$의 값이 ${ bf X}{ bf W}_2 +b_2$의 값보다 크므로 (3)번이 합리적임 . - 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 $ hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2$를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다. . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까) . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. (확률의 총합은 1이니까!) . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. . - 아래와 같은 활성화 함수를 도입하면 어떨까? . $$ hat{ boldsymbol{y}}=[ hat{ boldsymbol y}_1 ~ hat{ boldsymbol y}_2] = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big]$$ . - (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다 . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. --&gt; OK! . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. --&gt; OK! . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. --&gt; ??? . - 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고) . - 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자. . $$ hat{ boldsymbol y} = text{softmax}({ bf X} hat{ bf W} + { boldsymbol b}) = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big] $$ sigmoid, softmax . softmax&#45716; sigmoid&#51032; &#54869;&#51109;&#54805; . - 아래의 수식을 관찰하자. $$ frac{ exp( beta_0+ beta_1 x_i)}{1+ exp( beta_0+ beta_1x_i)}= frac{ exp( beta_0+ beta_1 x_i)}{e^0+ exp( beta_0+ beta_1x_i)}$$ . - 1을 $e^0$로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. . 모형2: ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 의 크기를 비교하고 확률 결정 | 모형1: ${ bf X} hat{ bf W} + hat{b}$ 와 $0$의 크기를 비교하고 확률 결정 = ${ bf X} hat{ bf W} + hat{b}$의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측 | . - 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다. . &#53364;&#47000;&#49828;&#51032; &#49688;&#44032; 2&#51064; &#44221;&#50864; softmax vs sigmoid . - 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 $y=0 text{ or } 1$와 같이 정리하지 않고 $y=[0,1] text{ or } [1,0]$ 와 같이 정리해도 무방할 듯 하다. . - 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 $y= 0 text{ or } 1$로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다. . - 표현력이 동등한 이유? 아래 수식을 관찰하자. . $$ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big) = big( frac{e^{0}}{e^{0}+e^{200}}, frac{e^{200}}{e^{0}+e^{200}} big)$$ . $ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big)$를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다. | $( hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2)$의 표현에서도 ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 라는 숫자 각각이 필요한 것이 아니고 $({ bf X} hat{ bf W}_1 + hat{b}_1)-({ bf X} hat{ bf W}_2 + hat{b}_2)$의 값만 알면 된다. | . - 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다. . &#48516;&#47448;&#54624; &#53364;&#47000;&#49828;&#44032; 3&#44060; &#51060;&#49345;&#51068; &#44221;&#50864; &#49888;&#44221;&#47581; &#47784;&#54805;&#51032; &#49444;&#44228; . - y의 모양: [0 1 0 0 0 0 0 0 0 0] . - 활성화함수의 선택: softmax . - 손실함수의 선택: cross entropy . Fashion_MNIST &#45796;&#51473;&#48516;&#47448; . - 데이터정리 . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . tf.keras.utils.to_categorical(y_train) . array([[0., 0., 0., ..., 0., 0., 1.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) . X=x_train.reshape(-1,784) y=tf.keras.utils.to_categorical(y_train) XX=x_test.reshape(-1,784) yy=tf.keras.utils.to_categorical(y_test) . - 시도1: 간단한 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y10&quot; &quot;node2&quot; -&gt; &quot;y10&quot; &quot;...&quot; -&gt; &quot;y10&quot; &quot;node30&quot; -&gt; &quot;y10&quot; &quot;node1&quot; -&gt; &quot;y1&quot; &quot;node2&quot; -&gt; &quot;y1&quot; &quot;...&quot; -&gt; &quot;y1&quot; &quot;node30&quot; -&gt; &quot;y1&quot; &quot;node1&quot; -&gt; &quot;.&quot; &quot;node2&quot; -&gt; &quot;.&quot; &quot;...&quot; -&gt; &quot;.&quot; &quot;node30&quot; -&gt; &quot;.&quot; label = &quot;Layer 2: softmax&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: softmax x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y10 y10 node1&#45;&gt;y10 y1 y1 node1&#45;&gt;y1 . . node1&#45;&gt;. node2&#45;&gt;y10 node2&#45;&gt;y1 node2&#45;&gt;. ...&#45;&gt;y10 ...&#45;&gt;y1 ...&#45;&gt;. node30&#45;&gt;y10 node30&#45;&gt;y1 node30&#45;&gt;. tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=20,batch_size=1200) . Epoch 1/20 50/50 [==============================] - 0s 831us/step - loss: 32.5672 - accuracy: 0.3730 Epoch 2/20 50/50 [==============================] - 0s 731us/step - loss: 2.4414 - accuracy: 0.3877 Epoch 3/20 50/50 [==============================] - 0s 881us/step - loss: 1.9154 - accuracy: 0.3809 Epoch 4/20 50/50 [==============================] - 0s 792us/step - loss: 1.7740 - accuracy: 0.4041 Epoch 5/20 50/50 [==============================] - 0s 830us/step - loss: 1.6474 - accuracy: 0.4517 Epoch 6/20 50/50 [==============================] - 0s 799us/step - loss: 1.5429 - accuracy: 0.4842 Epoch 7/20 50/50 [==============================] - 0s 738us/step - loss: 1.4649 - accuracy: 0.4992 Epoch 8/20 50/50 [==============================] - 0s 894us/step - loss: 1.4001 - accuracy: 0.5227 Epoch 9/20 50/50 [==============================] - 0s 829us/step - loss: 1.3400 - accuracy: 0.5461 Epoch 10/20 50/50 [==============================] - 0s 778us/step - loss: 1.2882 - accuracy: 0.5539 Epoch 11/20 50/50 [==============================] - 0s 883us/step - loss: 1.2506 - accuracy: 0.5612 Epoch 12/20 50/50 [==============================] - 0s 785us/step - loss: 1.2194 - accuracy: 0.5707 Epoch 13/20 50/50 [==============================] - 0s 766us/step - loss: 1.1915 - accuracy: 0.5732 Epoch 14/20 50/50 [==============================] - 0s 799us/step - loss: 1.1674 - accuracy: 0.5765 Epoch 15/20 50/50 [==============================] - 0s 732us/step - loss: 1.1453 - accuracy: 0.5808 Epoch 16/20 50/50 [==============================] - 0s 799us/step - loss: 1.1251 - accuracy: 0.5852 Epoch 17/20 50/50 [==============================] - 0s 805us/step - loss: 1.1047 - accuracy: 0.5886 Epoch 18/20 50/50 [==============================] - 0s 864us/step - loss: 1.0874 - accuracy: 0.5915 Epoch 19/20 50/50 [==============================] - 0s 830us/step - loss: 1.0716 - accuracy: 0.5944 Epoch 20/20 50/50 [==============================] - 0s 728us/step - loss: 1.0567 - accuracy: 0.5975 . &lt;keras.callbacks.History at 0x7fc80293a920&gt; . . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 675us/step - loss: 1.0963 - accuracy: 0.5930 . [1.0962964296340942, 0.5929999947547913] . - 시도2: 더 깊은 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node500&quot; &quot;x2&quot; -&gt; &quot;node500&quot; &quot;..&quot; -&gt; &quot;node500&quot; &quot;x784&quot; -&gt; &quot;node500&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;node1(2)&quot; &quot;node2&quot; -&gt; &quot;node1(2)&quot; &quot;...&quot; -&gt; &quot;node1(2)&quot; &quot;node500&quot; -&gt; &quot;node1(2)&quot; &quot;node1&quot; -&gt; &quot;node2(2)&quot; &quot;node2&quot; -&gt; &quot;node2(2)&quot; &quot;...&quot; -&gt; &quot;node2(2)&quot; &quot;node500&quot; -&gt; &quot;node2(2)&quot; &quot;node1&quot; -&gt; &quot;....&quot; &quot;node2&quot; -&gt; &quot;....&quot; &quot;...&quot; -&gt; &quot;....&quot; &quot;node500&quot; -&gt; &quot;....&quot; &quot;node1&quot; -&gt; &quot;node500(2)&quot; &quot;node2&quot; -&gt; &quot;node500(2)&quot; &quot;...&quot; -&gt; &quot;node500(2)&quot; &quot;node500&quot; -&gt; &quot;node500(2)&quot; label = &quot;Layer 2: relu&quot; } subgraph cluster_4{ style=filled; color=lightgrey; &quot;node1(2)&quot; -&gt; &quot;y10&quot; &quot;node2(2)&quot; -&gt; &quot;y10&quot; &quot;....&quot; -&gt; &quot;y10&quot; &quot;node500(2)&quot; -&gt; &quot;y10&quot; &quot;node1(2)&quot; -&gt; &quot;y1&quot; &quot;node2(2)&quot; -&gt; &quot;y1&quot; &quot;....&quot; -&gt; &quot;y1&quot; &quot;node500(2)&quot; -&gt; &quot;y1&quot; &quot;node1(2)&quot; -&gt; &quot;.&quot; &quot;node2(2)&quot; -&gt; &quot;.&quot; &quot;....&quot; -&gt; &quot;.&quot; &quot;node500(2)&quot; -&gt; &quot;.&quot; label = &quot;Layer 3: softmax&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: relu cluster_4 Layer 3: softmax x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node500 node500 x1&#45;&gt;node500 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node500 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node500 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node500 node1(2) node1(2) node1&#45;&gt;node1(2) node2(2) node2(2) node1&#45;&gt;node2(2) .... .... node1&#45;&gt;.... node500(2) node500(2) node1&#45;&gt;node500(2) node2&#45;&gt;node1(2) node2&#45;&gt;node2(2) node2&#45;&gt;.... node2&#45;&gt;node500(2) ...&#45;&gt;node1(2) ...&#45;&gt;node2(2) ...&#45;&gt;.... ...&#45;&gt;node500(2) node500&#45;&gt;node1(2) node500&#45;&gt;node2(2) node500&#45;&gt;.... node500&#45;&gt;node500(2) y10 y10 node1(2)&#45;&gt;y10 y1 y1 node1(2)&#45;&gt;y1 . . node1(2)&#45;&gt;. node2(2)&#45;&gt;y10 node2(2)&#45;&gt;y1 node2(2)&#45;&gt;. ....&#45;&gt;y10 ....&#45;&gt;y1 ....&#45;&gt;. node500(2)&#45;&gt;y10 node500(2)&#45;&gt;y1 node500(2)&#45;&gt;. tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=50,batch_size=1200) . Epoch 1/50 50/50 [==============================] - 0s 1ms/step - loss: 21.6114 - accuracy: 0.6873 Epoch 2/50 50/50 [==============================] - 0s 985us/step - loss: 1.9374 - accuracy: 0.8032 Epoch 3/50 50/50 [==============================] - 0s 948us/step - loss: 1.2284 - accuracy: 0.8216 Epoch 4/50 50/50 [==============================] - 0s 959us/step - loss: 0.9360 - accuracy: 0.8339 Epoch 5/50 50/50 [==============================] - 0s 1ms/step - loss: 0.7526 - accuracy: 0.8436 Epoch 6/50 50/50 [==============================] - 0s 986us/step - loss: 0.6205 - accuracy: 0.8538 Epoch 7/50 50/50 [==============================] - 0s 973us/step - loss: 0.5437 - accuracy: 0.8615 Epoch 8/50 50/50 [==============================] - 0s 984us/step - loss: 0.5181 - accuracy: 0.8644 Epoch 9/50 50/50 [==============================] - 0s 918us/step - loss: 0.4379 - accuracy: 0.8750 Epoch 10/50 50/50 [==============================] - 0s 948us/step - loss: 0.3880 - accuracy: 0.8852 Epoch 11/50 50/50 [==============================] - 0s 992us/step - loss: 0.3627 - accuracy: 0.8872 Epoch 12/50 50/50 [==============================] - 0s 917us/step - loss: 0.3536 - accuracy: 0.8899 Epoch 13/50 50/50 [==============================] - 0s 920us/step - loss: 0.3106 - accuracy: 0.9006 Epoch 14/50 50/50 [==============================] - 0s 939us/step - loss: 0.3013 - accuracy: 0.8997 Epoch 15/50 50/50 [==============================] - 0s 945us/step - loss: 0.2864 - accuracy: 0.9043 Epoch 16/50 50/50 [==============================] - 0s 998us/step - loss: 0.2813 - accuracy: 0.9067 Epoch 17/50 50/50 [==============================] - 0s 950us/step - loss: 0.2551 - accuracy: 0.9124 Epoch 18/50 50/50 [==============================] - 0s 920us/step - loss: 0.2427 - accuracy: 0.9155 Epoch 19/50 50/50 [==============================] - 0s 992us/step - loss: 0.2200 - accuracy: 0.9227 Epoch 20/50 50/50 [==============================] - 0s 931us/step - loss: 0.2154 - accuracy: 0.9240 Epoch 21/50 50/50 [==============================] - 0s 954us/step - loss: 0.2073 - accuracy: 0.9267 Epoch 22/50 50/50 [==============================] - 0s 932us/step - loss: 0.1962 - accuracy: 0.9300 Epoch 23/50 50/50 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9255 Epoch 24/50 50/50 [==============================] - 0s 934us/step - loss: 0.1958 - accuracy: 0.9303 Epoch 25/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9326 Epoch 26/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9372 Epoch 27/50 50/50 [==============================] - 0s 981us/step - loss: 0.1792 - accuracy: 0.9355 Epoch 28/50 50/50 [==============================] - 0s 957us/step - loss: 0.1658 - accuracy: 0.9400 Epoch 29/50 50/50 [==============================] - 0s 978us/step - loss: 0.1527 - accuracy: 0.9444 Epoch 30/50 50/50 [==============================] - 0s 931us/step - loss: 0.1600 - accuracy: 0.9418 Epoch 31/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9473 Epoch 32/50 50/50 [==============================] - 0s 924us/step - loss: 0.1442 - accuracy: 0.9466 Epoch 33/50 50/50 [==============================] - 0s 948us/step - loss: 0.1478 - accuracy: 0.9452 Epoch 34/50 50/50 [==============================] - 0s 998us/step - loss: 0.1489 - accuracy: 0.9457 Epoch 35/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9502 Epoch 36/50 50/50 [==============================] - 0s 953us/step - loss: 0.1290 - accuracy: 0.9526 Epoch 37/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9530 Epoch 38/50 50/50 [==============================] - 0s 932us/step - loss: 0.1202 - accuracy: 0.9556 Epoch 39/50 50/50 [==============================] - 0s 953us/step - loss: 0.1151 - accuracy: 0.9571 Epoch 40/50 50/50 [==============================] - 0s 977us/step - loss: 0.1089 - accuracy: 0.9601 Epoch 41/50 50/50 [==============================] - 0s 948us/step - loss: 0.1085 - accuracy: 0.9596 Epoch 42/50 50/50 [==============================] - 0s 963us/step - loss: 0.1139 - accuracy: 0.9583 Epoch 43/50 50/50 [==============================] - 0s 969us/step - loss: 0.1051 - accuracy: 0.9609 Epoch 44/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9582 Epoch 45/50 50/50 [==============================] - 0s 961us/step - loss: 0.1255 - accuracy: 0.9532 Epoch 46/50 50/50 [==============================] - 0s 914us/step - loss: 0.1059 - accuracy: 0.9614 Epoch 47/50 50/50 [==============================] - 0s 947us/step - loss: 0.1127 - accuracy: 0.9583 Epoch 48/50 50/50 [==============================] - 0s 921us/step - loss: 0.1059 - accuracy: 0.9608 Epoch 49/50 50/50 [==============================] - 0s 935us/step - loss: 0.1201 - accuracy: 0.9572 Epoch 50/50 50/50 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9622 . &lt;keras.callbacks.History at 0x7fc8014a3fa0&gt; . . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 697us/step - loss: 0.7534 - accuracy: 0.8623 . [0.7533721923828125, 0.8622999787330627] . &#49689;&#51228; . - 시도1,2에서 사용된 모형에서 각각 추정해야할 파라메터의 수를 구하고 비교하라. . hint: net.summary() 이용 .",
            "url": "https://guebin.github.io/DS2022/2022/05/04/(10%EC%A3%BC%EC%B0%A8)-5%EC%9B%944%EC%9D%BC.html",
            "relUrl": "/2022/05/04/(10%EC%A3%BC%EC%B0%A8)-5%EC%9B%944%EC%9D%BC.html",
            "date": " • May 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(9주차) 5월2일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tf.config.experimental.list_physical_devices() . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;)] . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . &#51473;&#44036;&#44256;&#49324; &#44288;&#47144; &#51105;&#45812; . &#51473;&#44036;&#44256;&#49324; 3&#48264;&#47928;&#51228; . - 특이한모형: 오버핏이 일어날 수 없는 모형이다. . 유의미한 coef: 상수항(bias), $ cos(t)$의 계수, $ cos(2t)$의 계수, $ cos(5t)$의 계수. | 유의미하지 않은 coef: $ cos(3t)$의 계수, $ cos(4t)$의 계수 | 유의미하지 않은 계수는 $n%$이 커질수록 0으로 추정된다 = $ cos(3t)$와 $ cos(5t)$는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌 | . - 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. . 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 $ beta$를 좀 더 정확히 추론하는게 차라리 더 이득) | 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다) | . - 직교기저의 예시 . 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. | 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. | 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $ text{color}= text{red}* beta_1 + text{blue}* beta_2 + text{yellow}* beta_3$ 이다. | (빨,파,노)는 색을 표현하는 basis이다. (적절한 $ beta_1, beta_2, beta_3$을 구하기만 하면 임의의 색도 표현가능) | (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) | (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) | (빨,파,노)는 직교기저이다. | . - 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함 . &#51473;&#44036;&#44256;&#49324; 1-(3)&#48264; &#47928;&#51228; . - 그림을 그려보자. . _x= tf.constant(np.arange(1,10001)/10000) _y= tnp.random.randn(10000) + (0.5 + 2*_x) plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7efe0046c580&gt;] . - 저것 꼭 10000개 다 모아서 loss계산해야할까? . plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) plt.plot(_x[::10],_y[::10],&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7efe00376850&gt;] . - 대충 이정도만 모아서 해도 비슷하지 않을까? $ to$ 해보자! . &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; . - 10개의 샘플이 있다고 가정. $ {(x_i,y_i) }_{i=1}^{10}$ . ver1: &#47784;&#46304; &#49368;&#54540;&#51012; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . ... . ver2: &#54616;&#45208;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . ver3: $m( leq n)$&#44060;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . &#50715;&#45216; (&#51328; &#45908; &#50628;&#48128;) . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . &#50836;&#51608; . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . note: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로 . ver1,2,3 &#51060;&#50808;&#50640; &#51328; &#45908; &#51648;&#51200;&#48516;&#54620; &#44163;&#46308;&#51060; &#51080;&#45796;. . - ver2,3에서 샘플을 셔플할 수도 있다. . - ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다. . - 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다. . Discussion . - 핵심개념 . 메모리사용량: ver1 &gt; ver3 &gt; ver2 | 계산속도: ver1 &gt; ver3 &gt; ver2 | local-min에 갇힘: ver1 &gt; ver3 &gt; ver2 | . - 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다. . - 틀리진 않지만 어색한 블로그 정리 내용들 . 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 어쩌다 보니까 확률적 경사하강법이 더 잘 빠져나오는것 | 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 계산이 빠르단 의미는 1회 업데이트 하는 속도가 빠르다는 의미임. 최종적으루 수렴을 빨리시키는지는 미지수임. 이것은 돌려봐야 안다. | . fashion_mnist &#47784;&#46280; . tf.keras.datasets.fashion_mnist.load_data() . type(tf.keras.datasets.fashion_mnist) . module . type(tf.keras.datasets.fashion_mnist.load_data) . function . &#45936;&#51060;&#53552;&#49373;&#49457; &#48143; &#53456;&#49353; . - tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성 . tf.keras.datasets.fashion_mnist.load_data?? . Signature: tf.keras.datasets.fashion_mnist.load_data() Source: @keras_export(&#39;keras.datasets.fashion_mnist.load_data&#39;) def load_data(): &#34;&#34;&#34;Loads the Fashion-MNIST dataset. This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The classes are: | Label | Description | |:--:|-| | 0 | T-shirt/top | | 1 | Trouser | | 2 | Pullover | | 3 | Dress | | 4 | Coat | | 5 | Sandal | | 6 | Shirt | | 7 | Sneaker | | 8 | Bag | | 9 | Ankle boot | Returns: Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`. **x_train**: uint8 NumPy array of grayscale image data with shapes `(60000, 28, 28)`, containing the training data. **y_train**: uint8 NumPy array of labels (integers in range 0-9) with shape `(60000,)` for the training data. **x_test**: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. **y_test**: uint8 NumPy array of labels (integers in range 0-9) with shape `(10000,)` for the test data. Example: python (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() assert x_train.shape == (60000, 28, 28) assert x_test.shape == (10000, 28, 28) assert y_train.shape == (60000,) assert y_test.shape == (10000,) License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the [MIT license]( https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE). &#34;&#34;&#34; dirname = os.path.join(&#39;datasets&#39;, &#39;fashion-mnist&#39;) base = &#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/&#39; files = [ &#39;train-labels-idx1-ubyte.gz&#39;, &#39;train-images-idx3-ubyte.gz&#39;, &#39;t10k-labels-idx1-ubyte.gz&#39;, &#39;t10k-images-idx3-ubyte.gz&#39; ] paths = [] for fname in files: paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname)) with gzip.open(paths[0], &#39;rb&#39;) as lbpath: y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[1], &#39;rb&#39;) as imgpath: x_train = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28) with gzip.open(paths[2], &#39;rb&#39;) as lbpath: y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[3], &#39;rb&#39;) as imgpath: x_test = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28) return (x_train, y_train), (x_test, y_test) File: ~/anaconda3/envs/tfcpu/lib/python3.9/site-packages/keras/datasets/fashion_mnist.py Type: function . - 함수의 return값을 확인하면 (x_train, y_train), (x_test, y_test)로 결과를 받아야 코드가 예뻐짐을 알 수 있다. . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . - 데이터의 확인 . x_train.shape, y_train.shape . ((60000, 28, 28), (60000,)) . x는 60000개의 관측치(이미지의 수)를 가지고 있는듯 보인다. | 하나의 obs에 대한 x의 차원은 (28,28)이다. | 하나의 obs에 대한 y의 차원은 스칼라다. | . x_test.shape, y_test.shape . ((10000, 28, 28), (10000,)) . train과 test의 비율을 6:1로 나눔 | . - 하나의 관측치가 무엇을 의미할까? . plt.imshow(x_train[0]) . &lt;matplotlib.image.AxesImage at 0x7efe002ab880&gt; . 신발? | . y_train[0] . 9 . 신발이미지=x, 신발임의의미하는숫자9=y | . np.where(y_train == 9) . (array([ 0, 11, 15, ..., 59932, 59970, 59978]),) . y==9인 다른 obs조사 | . plt.imshow(x_train[11]) . &lt;matplotlib.image.AxesImage at 0x7efe0021e3d0&gt; . obs0과 유사함 | . y_train . array([9, 0, 0, ..., 3, 0, 5], dtype=uint8) . &#45936;&#51060;&#53552;&#44396;&#51312; . x_train.shape . (60000, 28, 28) . - $ bf{X}$: (n,28,28), 픽셀크기가 28$ times$28 인 이미지 . - ${ bf y}$: (n,), 이미지에 대응하는 라벨 (0~9까지의 숫자로 되어있다) . &#50696;&#51228;1 . &#45936;&#51060;&#53552; &#51221;&#47532; . - y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니깐) . y=y_train[(y_train == 0) | (y_train == 1)].reshape(-1,1) X=x_train[(y_train == 0) | (y_train == 1)].reshape(-1,784) . yy= y_test[(y_test == 0) | (y_test == 1)].reshape(-1,1) XX= x_test[(y_test == 0) | (y_test== 1)].reshape(-1,784) . X.shape,y.shape, XX.shape,yy.shape . ((12000, 784), (12000, 1), (2000, 784), (2000, 1)) . &#54400;&#51060;1: &#51008;&#45769;&#52789;&#51012; &#54252;&#54632;&#54620; &#49888;&#44221;&#47581; // epochs=100 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 150ms/step - loss: 220.9145 Epoch 2/100 1/1 [==============================] - 0s 9ms/step - loss: 6800.3174 Epoch 3/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7045 Epoch 4/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7012 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7004 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6997 Epoch 7/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6991 Epoch 8/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6985 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6979 Epoch 10/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6976 Epoch 11/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6973 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6970 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6968 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6966 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6964 Epoch 16/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6963 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6961 Epoch 18/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6959 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6958 Epoch 20/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6956 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6955 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6953 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6952 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6951 Epoch 25/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6949 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6948 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6947 Epoch 28/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6946 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6945 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6944 Epoch 31/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6943 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6942 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6942 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6941 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6940 Epoch 36/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6940 Epoch 37/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6939 Epoch 38/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6939 Epoch 39/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6938 Epoch 40/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6937 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6937 Epoch 42/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6936 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6936 Epoch 44/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6935 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6935 Epoch 46/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6934 Epoch 47/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6934 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6934 Epoch 49/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 50/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 51/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 52/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 53/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 54/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 55/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 56/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 58/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 60/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 61/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 62/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 63/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 65/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 66/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 67/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 69/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 70/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 71/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 72/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6932 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 74/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 75/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 76/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 77/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 79/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 84/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 85/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 86/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 87/100 1/1 [==============================] - 0s 7ms/step - loss: 0.6932 Epoch 88/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 89/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 90/100 1/1 [==============================] - 0s 7ms/step - loss: 0.6932 Epoch 91/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 92/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 93/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 95/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 97/100 1/1 [==============================] - 0s 7ms/step - loss: 0.6932 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 . &lt;keras.callbacks.History at 0x7efe760c9d00&gt; . . np.mean((net(X)&gt;0.5) == y) . 0.5000833333333333 . np.mean((net(XX)&gt;0.5) == yy) . 0.5 . &#54400;&#51060;2: &#50741;&#54000;&#47560;&#51060;&#51200; &#44060;&#49440; . tf.random.set_seed(43055) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 143ms/step - loss: 100.9425 Epoch 2/100 1/1 [==============================] - 0s 9ms/step - loss: 44.4441 Epoch 3/100 1/1 [==============================] - 0s 9ms/step - loss: 29.2322 Epoch 4/100 1/1 [==============================] - 0s 9ms/step - loss: 22.6921 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 8.7741 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 4.6409 Epoch 7/100 1/1 [==============================] - 0s 9ms/step - loss: 5.2642 Epoch 8/100 1/1 [==============================] - 0s 9ms/step - loss: 6.1993 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 6.5543 Epoch 10/100 1/1 [==============================] - 0s 9ms/step - loss: 6.3454 Epoch 11/100 1/1 [==============================] - 0s 9ms/step - loss: 5.7887 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 5.1074 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 4.4821 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 3.9864 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 3.6388 Epoch 16/100 1/1 [==============================] - 0s 9ms/step - loss: 3.4077 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 3.2687 Epoch 18/100 1/1 [==============================] - 0s 8ms/step - loss: 3.1829 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 3.1196 Epoch 20/100 1/1 [==============================] - 0s 8ms/step - loss: 3.0526 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 2.9635 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 2.8397 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 2.6812 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 2.4916 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 2.2804 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 2.0630 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 1.8600 Epoch 28/100 1/1 [==============================] - 0s 8ms/step - loss: 1.6744 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 1.5003 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 1.3529 Epoch 31/100 1/1 [==============================] - 0s 8ms/step - loss: 1.2575 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 1.1763 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0853 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9978 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9337 Epoch 36/100 1/1 [==============================] - 0s 8ms/step - loss: 0.8893 Epoch 37/100 1/1 [==============================] - 0s 8ms/step - loss: 0.8503 Epoch 38/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8154 Epoch 39/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7843 Epoch 40/100 1/1 [==============================] - 0s 8ms/step - loss: 0.7548 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7288 Epoch 42/100 1/1 [==============================] - 0s 8ms/step - loss: 0.7061 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6844 Epoch 44/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6640 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6427 Epoch 46/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6187 Epoch 47/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5933 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5693 Epoch 49/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5471 Epoch 50/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5253 Epoch 51/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5031 Epoch 52/100 1/1 [==============================] - 0s 9ms/step - loss: 0.4805 Epoch 53/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4572 Epoch 54/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4368 Epoch 55/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4180 Epoch 56/100 1/1 [==============================] - 0s 8ms/step - loss: 0.3991 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3828 Epoch 58/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3701 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3568 Epoch 60/100 1/1 [==============================] - 0s 8ms/step - loss: 0.3426 Epoch 61/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3286 Epoch 62/100 1/1 [==============================] - 0s 8ms/step - loss: 0.3165 Epoch 63/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3051 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2929 Epoch 65/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2836 Epoch 66/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2753 Epoch 67/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2664 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2574 Epoch 69/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2493 Epoch 70/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2411 Epoch 71/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2335 Epoch 72/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2267 Epoch 73/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2198 Epoch 74/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2137 Epoch 75/100 1/1 [==============================] - 0s 10ms/step - loss: 0.2075 Epoch 76/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2013 Epoch 77/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1954 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1903 Epoch 79/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1850 Epoch 80/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1801 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1751 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1708 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1671 Epoch 84/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1629 Epoch 85/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1592 Epoch 86/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1553 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1514 Epoch 88/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1479 Epoch 89/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1441 Epoch 90/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1409 Epoch 91/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1373 Epoch 92/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1340 Epoch 93/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1305 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1275 Epoch 95/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1242 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1213 Epoch 97/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1184 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1157 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1132 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1110 . &lt;keras.callbacks.History at 0x7efe756ece50&gt; . . np.mean((net(X)&gt;0.5) == y) . 0.99175 . np.mean((net(XX)&gt;0.5) == yy) . 0.98 . &#54400;&#51060;3: &#52980;&#54028;&#51068;&#49884; metrics=[&#39;accuracy&#39;] &#52628;&#44032; . tf.random.set_seed(43055) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 234ms/step - loss: 100.9425 - accuracy: 0.4988 Epoch 2/100 1/1 [==============================] - 0s 9ms/step - loss: 44.4441 - accuracy: 0.3741 Epoch 3/100 1/1 [==============================] - 0s 8ms/step - loss: 29.2322 - accuracy: 0.4321 Epoch 4/100 1/1 [==============================] - 0s 9ms/step - loss: 22.6921 - accuracy: 0.5399 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 8.7741 - accuracy: 0.7321 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 4.6409 - accuracy: 0.8516 Epoch 7/100 1/1 [==============================] - 0s 10ms/step - loss: 5.2642 - accuracy: 0.8711 Epoch 8/100 1/1 [==============================] - 0s 9ms/step - loss: 6.1993 - accuracy: 0.8771 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 6.5543 - accuracy: 0.8845 Epoch 10/100 1/1 [==============================] - 0s 10ms/step - loss: 6.3454 - accuracy: 0.8953 Epoch 11/100 1/1 [==============================] - 0s 9ms/step - loss: 5.7887 - accuracy: 0.9062 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 5.1074 - accuracy: 0.9168 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 4.4821 - accuracy: 0.9276 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 3.9864 - accuracy: 0.9359 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 3.6388 - accuracy: 0.9402 Epoch 16/100 1/1 [==============================] - 0s 10ms/step - loss: 3.4077 - accuracy: 0.9412 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 3.2687 - accuracy: 0.9415 Epoch 18/100 1/1 [==============================] - 0s 9ms/step - loss: 3.1829 - accuracy: 0.9404 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 3.1196 - accuracy: 0.9388 Epoch 20/100 1/1 [==============================] - 0s 9ms/step - loss: 3.0526 - accuracy: 0.9380 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 2.9635 - accuracy: 0.9371 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 2.8397 - accuracy: 0.9376 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 2.6812 - accuracy: 0.9389 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 2.4916 - accuracy: 0.9396 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 2.2804 - accuracy: 0.9408 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 2.0630 - accuracy: 0.9433 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 1.8600 - accuracy: 0.9470 Epoch 28/100 1/1 [==============================] - 0s 9ms/step - loss: 1.6744 - accuracy: 0.9488 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 1.5003 - accuracy: 0.9510 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 1.3529 - accuracy: 0.9531 Epoch 31/100 1/1 [==============================] - 0s 9ms/step - loss: 1.2575 - accuracy: 0.9542 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 1.1763 - accuracy: 0.9553 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0853 - accuracy: 0.9567 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9978 - accuracy: 0.9587 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9337 - accuracy: 0.9603 Epoch 36/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8893 - accuracy: 0.9617 Epoch 37/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8503 - accuracy: 0.9627 Epoch 38/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8154 - accuracy: 0.9632 Epoch 39/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7843 - accuracy: 0.9642 Epoch 40/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7548 - accuracy: 0.9654 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7288 - accuracy: 0.9663 Epoch 42/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7061 - accuracy: 0.9674 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6844 - accuracy: 0.9687 Epoch 44/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6640 - accuracy: 0.9693 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6427 - accuracy: 0.9710 Epoch 46/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6187 - accuracy: 0.9716 Epoch 47/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5933 - accuracy: 0.9723 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5693 - accuracy: 0.9730 Epoch 49/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5471 - accuracy: 0.9733 Epoch 50/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5253 - accuracy: 0.9737 Epoch 51/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5031 - accuracy: 0.9744 Epoch 52/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4805 - accuracy: 0.9750 Epoch 53/100 1/1 [==============================] - 0s 9ms/step - loss: 0.4572 - accuracy: 0.9767 Epoch 54/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4368 - accuracy: 0.9775 Epoch 55/100 1/1 [==============================] - 0s 9ms/step - loss: 0.4180 - accuracy: 0.9778 Epoch 56/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3991 - accuracy: 0.9783 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3828 - accuracy: 0.9791 Epoch 58/100 1/1 [==============================] - 0s 8ms/step - loss: 0.3701 - accuracy: 0.9793 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3568 - accuracy: 0.9793 Epoch 60/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3426 - accuracy: 0.9805 Epoch 61/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3286 - accuracy: 0.9813 Epoch 62/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3165 - accuracy: 0.9822 Epoch 63/100 1/1 [==============================] - 0s 9ms/step - loss: 0.3051 - accuracy: 0.9827 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2929 - accuracy: 0.9827 Epoch 65/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2836 - accuracy: 0.9827 Epoch 66/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2753 - accuracy: 0.9830 Epoch 67/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2664 - accuracy: 0.9835 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2574 - accuracy: 0.9843 Epoch 69/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2493 - accuracy: 0.9845 Epoch 70/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2411 - accuracy: 0.9852 Epoch 71/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.9852 Epoch 72/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2267 - accuracy: 0.9852 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2198 - accuracy: 0.9858 Epoch 74/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2137 - accuracy: 0.9866 Epoch 75/100 1/1 [==============================] - 0s 9ms/step - loss: 0.2075 - accuracy: 0.9866 Epoch 76/100 1/1 [==============================] - 0s 8ms/step - loss: 0.2013 - accuracy: 0.9873 Epoch 77/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1954 - accuracy: 0.9875 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1903 - accuracy: 0.9879 Epoch 79/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1850 - accuracy: 0.9883 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1801 - accuracy: 0.9882 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1751 - accuracy: 0.9886 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1708 - accuracy: 0.9893 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1671 - accuracy: 0.9893 Epoch 84/100 1/1 [==============================] - 0s 9ms/step - loss: 0.1629 - accuracy: 0.9893 Epoch 85/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1592 - accuracy: 0.9898 Epoch 86/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1553 - accuracy: 0.9898 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1514 - accuracy: 0.9898 Epoch 88/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9900 Epoch 89/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1441 - accuracy: 0.9900 Epoch 90/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1409 - accuracy: 0.9899 Epoch 91/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1373 - accuracy: 0.9902 Epoch 92/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1340 - accuracy: 0.9902 Epoch 93/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1305 - accuracy: 0.9904 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1275 - accuracy: 0.9909 Epoch 95/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1242 - accuracy: 0.9908 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1213 - accuracy: 0.9908 Epoch 97/100 1/1 [==============================] - 0s 7ms/step - loss: 0.1184 - accuracy: 0.9910 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1157 - accuracy: 0.9911 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1132 - accuracy: 0.9912 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.1110 - accuracy: 0.9917 . &lt;keras.callbacks.History at 0x7efe74e40d90&gt; . . net.evaluate(X,y) . 375/375 [==============================] - 0s 363us/step - loss: 0.1086 - accuracy: 0.9918 . [0.10858087986707687, 0.9917500019073486] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 589us/step - loss: 0.2933 - accuracy: 0.9800 . [0.29328083992004395, 0.9800000190734863] . &#54400;&#51060;4: &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; &#51060;&#50857; // epochs=10 . tf.random.set_seed(43055) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=10,batch_size=120) . Epoch 1/10 100/100 [==============================] - 0s 848us/step - loss: 3.5696 - accuracy: 0.9374 Epoch 2/10 100/100 [==============================] - 0s 748us/step - loss: 0.3958 - accuracy: 0.9795 Epoch 3/10 100/100 [==============================] - 0s 830us/step - loss: 0.2030 - accuracy: 0.9862 Epoch 4/10 100/100 [==============================] - 0s 737us/step - loss: 0.1828 - accuracy: 0.9853 Epoch 5/10 100/100 [==============================] - 0s 763us/step - loss: 0.1633 - accuracy: 0.9863 Epoch 6/10 100/100 [==============================] - 0s 737us/step - loss: 0.0990 - accuracy: 0.9891 Epoch 7/10 100/100 [==============================] - 0s 683us/step - loss: 0.0633 - accuracy: 0.9908 Epoch 8/10 100/100 [==============================] - 0s 742us/step - loss: 0.0641 - accuracy: 0.9907 Epoch 9/10 100/100 [==============================] - 0s 731us/step - loss: 0.0693 - accuracy: 0.9894 Epoch 10/10 100/100 [==============================] - 0s 783us/step - loss: 0.0316 - accuracy: 0.9923 . &lt;keras.callbacks.History at 0x7efe7581ce50&gt; . . net.evaluate(X,y) . 375/375 [==============================] - 0s 346us/step - loss: 0.0360 - accuracy: 0.9919 . [0.03597424924373627, 0.9919166564941406] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 497us/step - loss: 0.1805 - accuracy: 0.9810 . [0.18054614961147308, 0.9810000061988831] .",
            "url": "https://guebin.github.io/DS2022/2022/05/02/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC.html",
            "relUrl": "/2022/05/02/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(9주차) 4월27일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . &#50864;&#46020;&#54632;&#49688;&#50752; &#52572;&#45824;&#50864;&#46020;&#52628;&#51221;&#47049; . (예제) . $X_i overset{iid}{ sim} Ber(p)$에서 얻은 샘플이 아래와 같다고 하자. . x=[0,1,0,1] x . [0, 1, 0, 1] . $p$는 얼마라고 볼 수 있는가? --&gt; 0.5 . 왜?? $p$가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가? . - suppose: $p=0.1$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.9 * 0.1 * 0.9 * 0.1 . 0.008100000000000001 . - suppose: $p=0.2$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.8 * 0.2 * 0.8 * 0.2 . 0.025600000000000008 . - 질문1: $p=0.1$인것 같냐? 아니면 $p=0.2$인것 같냐? -&gt; $p=0.2$ . 왜?? $p=0.2$일 확률이 더 크다! | . . (여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다. . - 0.0256은 &quot;$p=0.2$일 경우 샘플 (0,1,0,1)이 얻어질 확률&quot;이지 &quot;$p=0.2$일 확률&quot;은 아니다. . &quot;$p=0.2$인 확률&quot; 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다) . _plist = np.linspace(0.499,0.501,1000) sum([(1-p)*p*(1-p)*p for p in _plist]) . 62.49983299986714 . - 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -&gt; 가능도라는 말을 쓰자. . 0.0256 $=$ $p$가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 $=$ $p$가 0.2일 가능도 | . . - 다시 질문1로 돌아가자! . 질문1: $p=0.1$인 것 같냐? 아니면 $p=0.2$인 것 같냐? -&gt; 답 $p=0.2$ -&gt; 왜? $p=0.2$인 가능도가 더 크니까! | 질문2: $p=0.2$인 것 같냐? 아니면 $p=0.3$인 것 같냐? -&gt; 답 $p=0.3$ -&gt; 왜? $p=0.3$인 가능도가 더 크니까! | 질문3: ... | . - 궁극의 질문: $p$가 뭐일 것 같아? . $p$가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. | 그 함수를 최대화하는 $p$를 찾자. | 그 $p$가 궁극의 질문에 대한 대답이 된다. | . - 잠깐 용어정리 . 가능도함수 $=$ 우도함수 $=$ likelihood function $:=$ $L(p)$ | $p$의 maximum likelihood estimator $=$ p의 MLE $:=$ $ hat{p}^{mle}$ $=$ $ text{argmax}_p L(p)$ $=$ $ hat{p}$ | . (예제의 풀이) . - 이 예제의 경우 가능도함수를 정의하자. . $L(p)$: $p$의 가능도함수 = $p$가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = $p$가 모수일때 $x_1$이 0일 확률 $ times dots times$ $p$가 모수일때 $x_4$가 1일 확률 | $L(p)= prod_{i=1}^{4} f(x_i;p)= prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}$ | . . Note: 참고로 이 과정을 일반화 하면 $X_1, dots,X_n overset{iid}{ sim} Ber(p)$ 일때 $p$의 likelihood function은 $ prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 라고 볼 수 있다. . . Note: 더 일반화: $x_1, dots,x_n$이 pdf가 $f(x)$인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 $ prod_{i=1}^{n}f(x_i)$라고 볼 수 있다. . - 이 예제의 경우 $p$의 최대우도추정량을 구하면 . $$ hat{p}^{mle} = text{argmax}_p L(p) = text{argmax}_p big { p^2(1-p)^2 big }= frac{1}{2}$$ . &#51473;&#44036;&#44256;&#49324; 1&#48264; . (1) $N( mu, sigma)$에서 얻은 샘플이 아래와 같다고 할때 $ mu, sigma$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . (2) $Ber(p)$에서 얻은 샘플이 아래와 같다고 할 때 $p$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt; . (3) $y_i = beta_0 + beta_1 x_i + epsilon_i$, $ epsilon_i overset{iid}{ sim} N(0,1)$ 일때 $( beta_0, beta_1)$의 MLE를 구하여라. (회귀모형) . (풀이) 가능도함수 . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 를 최대화하는 $ beta_0, beta_1$을 구하면된다. . 그런데 이것은 아래를 최소화하는 $ beta_0, beta_1$을 구하는 것과 같다. . $$- log L( beta_0, beta_1) = sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$$ . 위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $ beta_0, beta_1$을 구하면 된다. . 중간고사 1-(3)의 다른 풀이 . step1: 생성 . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . step2: minimize MSEloss (원래는 maximize log-likelihood) . maximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 $ beta_0, beta_1$은 MSE를 최소화하는 $ beta_0, beta_1$과 동일하므로 | . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2) loss = tf.reduce_sum((y-beta0-beta1*x)**2) slope1, slope2 = tape.gradient(loss,[beta0,beta1]) beta0.assign_sub(slope1* 0.1/10000) # N=10000 beta1.assign_sub(slope2* 0.1/10000) . beta0,beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.49661896&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.0051448&gt;) . - 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? . 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함 | . (4) 출제하지 못한 중간고사 문제 . 아래의 모형을 생각하자. . $Y_i overset{iid}{ sim} Ber( pi_i)$ | $ pi_i = frac{exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}= frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ | . 아래는 위의 모형에서 얻은 샘플이다. . x = tnp.linspace(-1,1,2000) pi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x)) y = np.random.binomial(1,pi) y = tf.constant(y) . 함수 $L(w_0,w_1)$을 최대화하는 $(w_0,w_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $(w_0,w_1)$의 초기값은 모두 0.1로 설정할 것) . $$L(w_0,w_1)= prod_{i=1}^{n}f(y_i), quad f(x_i)={ pi_i}^{y_i}(1- pi_i)^{1-y_i}, quad pi_i= text{sigmoid}(w_0+w_1x_i)$$ . (풀이1) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) pdf = pihat**y * (1-pihat)**(1-y) logL = tf.reduce_mean(tnp.log(pdf)) slope1,slope2 = tape.gradient(logL,[w0hat,w1hat]) w0hat.assign_add(slope1*0.1) w1hat.assign_add(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.8875932&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.3785405&gt;) . (해석) - 로지스틱에서 가능도함수와 BCEloss의 관계 . $L(w_0,w_1)$를 최대화하는 $w_0,w_1$은 아래를 최소화하는 $w_0,w_1$와 같다. . $$- log L(w_0,w_1) = - sum_{i=1}^{n} big(y_i log( pi_i) + (1-y_i) log(1- pi_i) big)$$ . 이것은 최적의 $w_0,w_1$을 $ hat{w}_0, hat{w}_1$이라고 하면 $ hat{ pi}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= hat{y}_i$이 되고 따라서 위의 식은 $n times$BCEloss의 형태임을 쉽게 알 수 있다. . 결국 로지스틱 모형에서 $(w_0,w_1)$의 MLE를 구하기 위해서는 BCEloss를 최소화하는 $(w_0,w_1)$을 구하면 된다! . (풀이2) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) loss = tf.losses.binary_crossentropy(y,yhat) slope1,slope2 = tape.gradient(loss,[w0hat,w1hat]) w0hat.assign_sub(slope1*0.1) w1hat.assign_sub(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.8875934&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.3785415&gt;) . &#49552;&#49892;&#54632;&#49688;&#51032; &#49444;&#44228; (&#49440;&#53469;) . - 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. . 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 | 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다 | . - minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. &lt; 참고만하세요, 이 수업에서는 안중요합니다. . 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 $ hat{ beta}$은 여전히 MSEloss를 최소화하는 $ beta$를 구함으로써 얻을 수 있다. | 이 경우 MSEloss를 쓰는 이론적근거? $ hat{ beta}이 BLUE가 되기 때문임 (가우스-마코프정리) | .",
            "url": "https://guebin.github.io/DS2022/2022/04/28/(9%EC%A3%BC%EC%B0%A8)-4%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/04/28/(9%EC%A3%BC%EC%B0%A8)-4%EC%9B%9428%EC%9D%BC.html",
            "date": " • Apr 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022년 중간고사 해설",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . (풀이) . sigma = tf.Variable(3.0) mu = tf.Variable(2.0) . with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) tape.gradient(logL,[mu,sigma]) . [&lt;tf.Tensor: shape=(), dtype=float32, numpy=1129.3353&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431&gt;] . for i in range(1000): with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) slope1, slope2 = tape.gradient(logL,[mu,sigma]) mu.assign_add(slope1* 0.1/10000) # N=10000 sigma.assign_add(slope2* 0.1/10000) . mu,sigma . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.0163972&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9870595&gt;) . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L(p)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . (풀이) . p=tf.Variable(0.3) for i in range(1000): with tf.GradientTape() as tape: pdf = p**x * (1-p)**(1-x) logL = tf.reduce_sum(tnp.log(pdf)) slope = tape.gradient(logL,p) p.assign_add(slope* 0.1/10000) # N=10000 . p . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.796&gt; . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . 함수 $L( beta_0, beta_1)$을 최대화하는 $( beta_0, beta_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ beta_0, beta_1$의 초기값은 모두 1로 설정할 것) . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . (풀이) . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: mu = beta0 + beta1*x pdf = tnp.exp(-0.5*(y-mu)**2) logL = tf.reduce_sum(tnp.log(pdf)) slope1, slope2 = tape.gradient(logL,[beta0,beta1]) beta0.assign_add(slope1* 0.1/10000) # N=10000 beta1.assign_add(slope2* 0.1/10000) . beta0, beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.5553082&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.8987025&gt;) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (풀이) . tf.linalg.inv(X.T @ X ) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457323], [2.21570461]])&gt; . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (풀이) . beta= tnp.array([5,10]).reshape(2,1) . for i in range(50000): beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10 . beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.28579424], [2.24168098]])&gt; . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) for i in range(50000): with tf.GradientTape() as tape: loss = (y-X@beta).T @ (y-X@beta) / 10 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) loss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 for i in range(50000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7f14d0ac6e00&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (풀이) . y = y.reshape(1000,1) x1 = np.cos(t) x2 = np.cos(2*t) x3 = np.cos(3*t) x4 = np.cos(4*t) x5 = np.cos(5*t) X = tf.stack([x1,x2,x3,x4,x5],axis=1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;sgd&#39;) net.fit(X,y,batch_size=1000, epochs = 1000, verbose=0) . &lt;keras.callbacks.History at 0x7f14d0ac6b00&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(5, 1) dtype=float32, numpy= array([[ 3.0008891e+00], [ 1.0066563e+00], [ 1.8562324e-03], [-3.8704993e-03], [ 4.9712175e-01]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)&gt;] . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7f14d0b01690&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . (풀이) . x= x.reshape(2000,1) y= y.reshape(2000,1) . net= tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;, loss= tf.losses.binary_crossentropy) net.fit(x,y,epochs=10000,batch_size=2000, verbose=0) . &lt;keras.callbacks.History at 0x7f14b818d840&gt; . net.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.201613]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.90172565], dtype=float32)&gt;] . plt.plot(y,&#39;.&#39;) plt.plot(net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14b3f06b30&gt;] . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7f14b3e7fb50&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . (풀이) . $ hat{ beta}_0= 0.6069048$ | $ hat{ beta}_1= 2.2616348$ | . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $yhat= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f14b3dc7490&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $yhat$를 만드는 코드를 작성하라. . (풀이) . u=net.layers[0](x) v=net.layers[1](u) yhat=net.layers[2](v) . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다. . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. .",
            "url": "https://guebin.github.io/DS2022/2022/04/27/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%ED%95%B4%EC%84%A4.html",
            "relUrl": "/2022/04/27/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%ED%95%B4%EC%84%A4.html",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2022 데이터 과학 중간고사",
            "content": "import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . 함수 $L( beta_0, beta_1)$을 최대화하는 $( beta_0, beta_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ beta_0, beta_1$의 초기값은 모두 1로 설정할 것) . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fceb6997340&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7fceb9941ed0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7f6b142800d0&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $y= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f6af6c39f30&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $y$를 만드는 코드를 작성하라. . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. .",
            "url": "https://guebin.github.io/DS2022/2022/04/25/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "relUrl": "/2022/04/25/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "중간고사 예상문제",
            "content": "import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . 2022-04-24 23:46:13.390830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. (10점) . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . hint: $L( mu, sigma)$를 최대화하는 $( mu, sigma)$는 $ log L( mu, sigma)$를 역시 최대화한다는 사실을 이용할 것. . hint: $ mu$의 참값은 3, $ sigma$의 참값은 2이다. (따라서 $ mu$와 $ sigma$는 각각 2와 3근처로 추정되어야 한다.) . (2) . (3) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . hint1 alpha=0.0015로 설정할 것 . hint2 epoc은 10000번정도 반복실행하며 적당한 횟수를 찾을 것 . hint3 (1)의 최적값에 반드시 정확히 수렴시킬 필요는 없음 (너무 많은 에폭이 소모됨) . hint4 초기값으로 [5,10] 정도 이용할 것 . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fceb6997340&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7fceb9941ed0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . 4. Piecewise-linear regression (15&#51216;) . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. . (2) . (3) . (4) . (5) . some notes . - 용어를 모르겠는 분은 질문하시기 바랍니다. . - 풀다가 에러나는 코드 질문하면 에러 수정해드립니다. .",
            "url": "https://guebin.github.io/DS2022/2022/04/23/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C.html",
            "relUrl": "/2022/04/23/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C.html",
            "date": " • Apr 23, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "시험관련 안내사항",
            "content": "&#49884;&#54744;&#50976;&#54805; . - 오픈북: 강의노트, 본인이 정리한 노트, 인터넷 검색 가능 . - 비대면: Zoom을 활용하여 응시 . &#49884;&#54744;&#49884;&#44036; . - 일시: LMS를 통해 공지한 날의 수업시간 . - 시험시간 중 처음 30분은 장비점검시간으로 활용함 (단, 모든 사람이 준비될 경우 30분을 기다리지 않고 시작) . 따라서 2시간 수업일 경우 실질적으로 문제를 풀고 답안을 제출할때 까지 쓸 수 있는 시간은 1시간30분입니다. | . &#49884;&#54744;&#49892; &#51077;&#51109; . - LMS $ to$ 강의대화 $ to$ Zoom 화상강의 바로 가기로 입장 . &#49884;&#54744;&#47928;&#51228; &#44277;&#44060;&#48169;&#49885; . - LMS에 주피터노트북 파일 업로드 + LMS 공지사항을 통하여 시험문제의 URL을 공개 . 주피터파일의 장점: URL보다 LMS의 주피터노트북이 더 빠르게 공개됨. | URL의 장점: 시험문제의 오류가 있을 경우 수정이후 URL에 반영됨. | . &#51228;&#52636; . - 답안지: LMS의 레포트 메뉴를 활용하여 답안지를 제출 (종료시간 이전에 미리 제출가능) . - 동영상: 시험시간동안 컴퓨터전체화면 녹화후 제출 . 듀얼모니터의 경우 작업표시줄이 나타나는 모니터를 녹화 | 아이패드와 동시사용시에는 아이패드도 함께 녹화 | . &#51456;&#48708;&#47932; . - 컴퓨터 및 노트북: 시험지 확인 및 문제풀이 용도 . - 핸드폰: Zoom을 통하여 주변상황을 및 컴퓨터 화면을 촬영하는 용도 . 중간에 핸드폰 및 노트북이 꺼지지 않도록 충전기를 준비한다. | . - 학생증 혹은 신분증 (본인확인용도) . &#49884;&#54744;&#51204; &#51456;&#48708;&#49324;&#54637; . - 시험준비시간 동안 핸드폰을 아래와 같이 배치하여 학생의 컴퓨터 화면 및 주변상황이 보이도록 함 . . 적절한 각도를 설정하기 어려운 경우 주변환경보다 컴퓨터의 화면이 잘 보이도록 설정할 것 | . - 학생증을 준비하여 시험 시작 직전에 본인의 얼굴과 학생증을 함께 촬영한다. (5초간) . &#50976;&#51032;&#49324;&#54637; . - 줌의 대화명은 이름과 학번을 모두 적는다. (예시: 최규빈_202143052) . 동명이인이 있을 수 있으므로 학번을 같이 적으세요 | . - 질문은 카카오톡 채널 혹은 줌의 채팅기능을 이용한다. . - Zoom에서 스피커 음소거를 하지 않는다. (전체 공지사항등이 있을때 음성으로 공지함) . - 핸드폰으로 Zoom참가 중 전화가 오면 거절하고 받지 않는다. (전화통화시 Zoom연결이 종료되므로 부정행위로 의심할 수 있음) . &#44592;&#53440; &#52280;&#44256;&#49324;&#54637; . - 핸드폰과 피씨를 이용하여 줌에 동시접속할 경우 . 최규빈_202143052_핸드폰 | 최규빈_202143052_컴퓨터 | . 와 같이 기기를 분리하여 적는다. . - 시험문제는 코랩으로 풀어도 무방하며 시험문제를 다운받아 개인 주피터노트북 등으로 풀어도 무방하다. . - 답안지 제출형식은 주피터 노트북파일을 권장한다. 하지만 풀이 및 코드를 알아볼 수 있는 어떠한 형식으로 제출하여도 무방하다. (ex: txt, hwp, pdf, html..) .",
            "url": "https://guebin.github.io/DS2022/2022/04/19/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "relUrl": "/2022/04/19/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "date": " • Apr 19, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(7주차) 4월18일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . piece-wise linear regression . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . np.random.seed(43052) N=100 x = np.linspace(-1,1,N) lamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3 y= np.array(list(map(lamb,x))) y . array([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569, -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671, -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354, -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 , -0.69867774, -0.34811037, 0.11188474, -1.05046296, -0.03840085, -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303, -0.529586 , -0.27814478, -0.10852781, -0.38294596, 0.02669763, -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793, -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763, 0.28917092, 0.18175206, -0.48988124, -0.08084459, 0.37706178, 0.14478908, 0.07621827, -0.071864 , 0.05143365, 0.33932009, -0.35071776, 0.87742867, 0.51370399, 0.34863976, 0.55855514, 1.14196717, 0.86421076, 0.72957843, 0.57342304, 1.54803332, 0.98840018, 1.11129366, 1.42410801, 1.44322465, 1.25926455, 1.12940772, 1.46516829, 1.16365096, 1.45560853, 1.9530553 , 2.45940445, 1.52921129, 1.8606463 , 1.86406718, 1.5866523 , 1.49033473, 2.35242686, 2.12246412, 2.41951931, 2.43615052, 1.96024441, 2.65843789, 2.46854394, 2.76381882, 2.78547462, 2.56568465, 3.15212157, 3.11482949, 3.17901774, 3.31268904, 3.60977818, 3.40949166, 3.30306495, 3.74590922, 3.85610433]) . plt.plot(x,y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3b7f1187f0&gt;] . &#54400;&#51060;1: &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; . x= x.reshape(N,1) y= y.reshape(N,1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . 2022-04-18 11:40:03.840482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;keras.callbacks.History at 0x7f3b7d0bf670&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . yhat = x * 2.2616348 + 0.6069048 yhat = net.predict(x) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3b7f163370&gt;] . - 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 . 왜? 아키텍처 설계자체가 틀렸음 | 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제 | . &#54400;&#51060;2: &#48708;&#49440;&#54805; &#54876;&#49457;&#54868; &#54632;&#49688;&#51032; &#46020;&#51077; . - 여기에서 비선형 활성화 함수는 relu . - 네트워크를 아래와 같이 수정하자. . (수정전) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, &#160;&#160;&#160;bias=True x*w, &#160;&#160;&#160;bias=True x&#45;&gt;x*w, &#160;&#160;&#160;bias=True *w y y x*w, &#160;&#160;&#160;bias=True&#45;&gt;y indentity (수정후) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;relu&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, &#160;&#160;&#160;bias=True x*w, &#160;&#160;&#160;bias=True x&#45;&gt;x*w, &#160;&#160;&#160;bias=True *w y y x*w, &#160;&#160;&#160;bias=True&#45;&gt;y relu 마지막에 $f(x)=x$ 라는 함수대신에 relu를 취하는 것으로 구조를 약간 변경 | 활성화함수(acitivation function)를 indentity에서 relu로 변경 | . - relu함수란? . _x = np.linspace(-1,1,100) tf.nn.relu(_x) . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909, 0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919, 0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929, 0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939, 0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949, 0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 , 0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 , 0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 , 0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 , 0.91919192, 0.93939394, 0.95959596, 0.97979798, 1. ])&gt; . plt.plot(_x,_x) plt.plot(_x,tf.nn.relu(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f3b7c1ab610&gt;] . 파란색을 주황색으로 바꿔주는 것이 렐루함수임 | $f(x)= max(0,x)= begin{cases} 0 &amp; x leq 0 x &amp; x&gt;0 end{cases}$ | . - 아키텍처: $ hat{y}_i=relu( hat{w}_0+ hat{w}_1x_i)$, $relu(x)= max(0,x)$ . - 풀이시작 . 1단계 . net2 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(1, input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net2.add(l1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3b5c6e74f0&gt;] . net2.add(a1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3b5c6e74f0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa99a0ee0&gt;] . l1.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . net2.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . (네트워크 상황 확인) . u1= l1(x) #u1= x@l1.weights[0] + l1.weights[1] . v1= a1(u1) #v1= tf.nn.relu(u1) . plt.plot(x,x) plt.plot(x,u1,&#39;--r&#39;) plt.plot(x,v1,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3aa98243a0&gt;] . 3단계 . net2.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) . 4단계 . net2.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f3aa9885990&gt; . - result . yhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) yhat = net2.predict(x) yhat = net2(x) yhat = a1(l1(x)) yhat = net2.layers[1](net2.layers[0](x)) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3aa636bee0&gt;] . - discussion . 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 $ to$ 모형의 표현력이 떨어진다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3: &#45432;&#46300;&#49688;&#52628;&#44032; + &#47112;&#51060;&#50612;&#52628;&#44032; . 목표: 2개의 주황색 점선을 만들자. . 1단계 . net3 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(2,input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net3.add(l1) net3.add(a1) . (네트워크 상황 확인) . l1(x).shape # l1(x) : (100,1) -&gt; (100,2) . TensorShape([100, 2]) . plt.plot(x,x) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c5f7730&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c5f7970&gt;] . plt.plot(x,x) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c48cf70&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c48d1b0&gt;] . - 이 상태에서는 yhat이 안나온다. 왜? . 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. | 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음. | . - 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! . yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1] | . - 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. . 입력차원: 2 | 출력차원: 1 | . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3aa62bb3d0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa62baad0&gt;] . tf.random.set_seed(43053) l2 = tf.keras.layers.Dense(1, input_shape=(2,)) . net3.add(l2) . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3aa62bb3d0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa62baad0&gt;, &lt;keras.layers.core.dense.Dense at 0x7f3aa61c3160&gt;] . net3.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 2) 4 activation_9 (Activation) (None, 2) 0 dense_11 (Dense) (None, 1) 3 ================================================================= Total params: 7 Trainable params: 7 Non-trainable params: 0 _________________________________________________________________ . - 추정해야할 파라메터수가 4,0,3으로 나온다. . - 수식표현: $X to X@W^{(1)}+b^{(1)} to relu(X@W^{(1)}+b^{(1)}) to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat$ . $X$: (N,1) | $W^{(1)}$: (1,2) ==&gt; 파라메터 2개 추정 | $b^{(1)}$: (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개 | $W^{(2)}$: (2,1) ==&gt; 파라메터 2개 추정 | $b^{(2)}$: (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개 | . - 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. . 초거대AI: 추정할 파라메터수가 엄청 많은.. | . net3.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . l1.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;] . l2.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . - 좀 더 간단한 수식표현: $X to (u_1 to v_1) to (u_2 to v_2) = yhat$ . $u_1= X@W^{(1)}+b^{(1)}$ | $v_1= relu(u_1)$ | $u_2= v_1@W^{(2)}+b^{(2)}$ | $v_2= indentity(u_2):=yhat$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat 3단계 . net3.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) . 4단계 . net3.fit(x,y,epochs=1000,verbose=0, batch_size=N) . &lt;keras.callbacks.History at 0x7f3aa61ba560&gt; . - 결과확인 . net3.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 1.6352799 , -0.85507524]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.08284465, 0.85552216], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328746], [-1.2001747]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253307], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c384040&gt;] . - 분석 . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c2bdde0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c2be020&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c131870&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c131ab0&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l2(a1(l1(x))),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c1ad480&gt;] . - 마지막 2개의 그림을 분석 . l2.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328746], [-1.2001747]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253307], dtype=float32)&gt;] . fig, (ax1,ax2,ax3) = plt.subplots(1,3) fig.set_figwidth(12) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax1.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0]*1.6328746,&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a71028c40&gt;] . &#54400;&#51060;3&#51032; &#49892;&#54056; . tf.random.set_seed(43054) ## 1단계 net3 = tf.keras.Sequential() ## 2단계 net3.add(tf.keras.layers.Dense(2)) net3.add(tf.keras.layers.Activation(&#39;relu&#39;)) net3.add(tf.keras.layers.Dense(1)) ## 3단계 net3.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) ## 4단계 net3.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f3a70c237c0&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a7032f700&gt;] . - 엥? 에폭이 부족한가? . net3.fit(x,y,epochs=10000,verbose=0,batch_size=N) plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a70278160&gt;] . - 실패분석 . l1,a1,l2 = net3.layers . l2.weights . [&lt;tf.Variable &#39;dense_13/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) fig.set_figwidth(16) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,l1(x)[:,0],&#39;--r&#39;) ax1.plot(x,l1(x)[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*0.65121335,&#39;--r&#39;) ax3.plot(x,a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--b&#39;) ax4.plot(x,y,&#39;.&#39;) ax4.plot(x,a1(l1(x))[:,0]*0.65121335+a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a7081c130&gt;] . 보니까 빨간색선이 하는 역할을 없음 | 그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음 | 왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 $ to$ 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가) | 즉 이 상황 자체가 나름 최적회된 상태이다. 이러한 현상을 &quot;global minimum을 찾지 못하고 local minimum에 빠졌다&quot;라고 표현한다. | . 확인: . net3.weights . [&lt;tf.Variable &#39;dense_12/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[-0.03077251, 1.8713338 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_12/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.04834982, 0.3259186 ], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . W1= tf.Variable(tnp.array([[-0.03077251, 1.8713338 ]])) b1= tf.Variable(tnp.array([-0.04834982, 0.3259186 ])) W2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]])) b2= tf.Variable(tnp.array([-0.60076195])) . with tf.GradientTape() as tape: u = tf.constant(x) @ W1 + b1 v = tf.nn.relu(u) yhat = v@W2 + b2 loss = tf.losses.mse(y,yhat) . tape.gradient(loss,[W1,b1,W2,b2]) . [&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;, &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ 0.00000000e+00], [-4.74910706e-05]])&gt;, &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;] . 예상대로 계수값이 거의 다 0이다. . &#54400;&#51060;4: &#45432;&#46300;&#49688;&#47484; &#45908; &#52628;&#44032;&#54620;&#45796;&#47732;? . - 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; &quot;X&quot; -&gt; &quot;...&quot; &quot;X&quot; -&gt; &quot;node512&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node512&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 ... ... X&#45;&gt;... node512 node512 X&#45;&gt;node512 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat ...&#45;&gt;yhat node512&#45;&gt;yhat tf.random.set_seed(43056) net4= tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) # 이렇게 해도됩니다. net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) net4.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f34ac815d20&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net4(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f34ac6d26b0&gt;] . 잘된다.. | 한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯! | . - 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit) . np.random.seed(43052) N=100 _x = np.linspace(0,1,N).reshape(N,1) _y = np.random.normal(loc=0,scale=0.001,size=(N,1)) plt.plot(_x,_y) . [&lt;matplotlib.lines.Line2D at 0x7f34ac4202b0&gt;] . tf.random.set_seed(43052) net4 = tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.5)) net4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f34ac2f3640&gt; . plt.plot(_x,_y) plt.plot(_x,net4(_x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f34ac07a1a0&gt;] . 이 예제는 추후 다시 공부할 예정 | . Logistic regression . motive . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 올라간다. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt;-- 외우세요!! . $y_i sim Ber( pi_i)$, where $ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i = frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ . | . - 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자) . &#50696;&#51228; . N = 2000 . x = tnp.linspace(-1,1,N).reshape(N,1) w0 = -1 w1 = 5 u = w0 + x*w1 #v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi v = tf.nn.sigmoid(u) y = tf.constant(np.random.binomial(1,v),dtype=tf.float64) . plt.plot(x,y,&#39;.&#39;,alpha=0.02) plt.plot(x,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349e22fa60&gt;] . - 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat sigmoid - 또는 간단하게 아래와 같이 쓸 수 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; x label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; x -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: sigmoid x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - 케라스를 이용하여 적합을 해보면 . $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ | . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) bceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat)) net.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f349c638670&gt; . net.weights . [&lt;tf.Variable &#39;dense_28/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.1423755]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_28/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.820938], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349df177c0&gt;] . MSE loss? . - mse loss를 쓰면 왜 안되는지? . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) mseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2) net.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f349df8cc10&gt; . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349ce42230&gt;] . 일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임 | . MSE loss vs BCE loss . - MSEloss, BCEloss의 시각화 . w0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing=&#39;ij&#39;) w0, w1 = w0.reshape(-1), w1.reshape(-1) def loss_fn1(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return mseloss_fn(y,yhat) def loss_fn2(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return bceloss_fn(y,yhat) loss1 = list(map(loss_fn1,w0,w1)) loss2 = list(map(loss_fn2,w0,w1)) . fig = plt.figure() fig.set_figwidth(9) fig.set_figheight(9) ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 ax1.scatter(w0,w1,loss1,s=0.1) ax2.scatter(w0,w1,loss2,s=0.1) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f34a457df00&gt; . 왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음 | . &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;1 . - 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 66ms/step - loss: 0.1554 . &lt;keras.callbacks.History at 0x7f349dad3760&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 76ms/step - loss: 0.9265 . &lt;keras.callbacks.History at 0x7f349da22fe0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.5575908], [ 1.1560522]], dtype=float32)], [array([[-0.8477989 ], [-0.91781974]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;SGD, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;2 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 79ms/step - loss: 0.2311 . &lt;keras.callbacks.History at 0x7f349d524070&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 94ms/step - loss: 0.5606 . &lt;keras.callbacks.History at 0x7f349d526fb0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[0.07441761], [0.40206426]], dtype=float32)], [array([[-0.86062825], [ 0.9297301 ]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;3 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 75ms/step - loss: 0.2175 . &lt;keras.callbacks.History at 0x7f349c9d9e40&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 82ms/step - loss: 0.5323 . &lt;keras.callbacks.History at 0x7f349caa17b0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.02143217], [ 0.484821 ]], dtype=float32)], [array([[-0.8675074], [ 1.1268172]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-10.], [ -1.]], dtype=float32)], [array([[-10.], [ -1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-10,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect 아무리 아담이라고 해도 이건 힘듬 | . - discussion . mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. | 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) | 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2). 그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3) | . - 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? . mse loss를 사용하면 손실함수가 convex하지 않으니까! | 그리고 bce loss를 사용하면 손실함수가 convex하니까! | .",
            "url": "https://guebin.github.io/DS2022/2022/04/18/(7%EC%A3%BC%EC%B0%A8)-4%EC%9B%9418%EC%9D%BC.html",
            "relUrl": "/2022/04/18/(7%EC%A3%BC%EC%B0%A8)-4%EC%9B%9418%EC%9D%BC.html",
            "date": " • Apr 18, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "(6주차) 4월11일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . $x to hat{y}$ &#44032; &#46104;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#44536;&#47532;&#44592; . - 단순회귀분석의 예시 . $ hat{y}_i = hat{ beta}_0 + hat{ beta}_1 x_i, quad i=1,2, dots,n$ | . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xₙ&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xₙ*β̂₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;identity&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₂&quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₂*β̂₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;identity&quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₁&quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₁*β̂₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷₙ ŷₙ β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷₙ identity xₙ xₙ xₙ&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ . . .................................... .................................... .&#45;&gt;.................................... * β̂₀ ... ... ....................................&#45;&gt;... .. .. ..&#45;&gt;.................................... * β̂₁ 1 1 β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False 1 &#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₂ ŷ₂ β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₂ identity x₂ x₂ x₂&#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ 1 &#160; 1 &#160; β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False 1 &#160;&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₁ ŷ₁ β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₁ identity x₁ x₁ x₁&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ - 표현1의 소감? . 교수님이 고생해서 만든것 같음 | 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음. | . (표현2) . - 그냥 아래와 같이 그리고 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot;고 하면 될것 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xᵢ&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷᵢ ŷᵢ β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷᵢ identity xᵢ xᵢ xᵢ&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현3) . - 그런데 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot; 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x*β̂₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ ŷ β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity x x x&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현4) . - 위의 모델은 아래와 같이 쓸 수 있다. ($ beta_0$를 바이어스로 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*β̂₁, bias=True&quot;[label=&quot;*β̂₁&quot;] ; &quot;x*β̂₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*β̂₁, &#160;&#160;&#160;bias=True x*β̂₁, &#160;&#160;&#160;bias=True x&#45;&gt;x*β̂₁, &#160;&#160;&#160;bias=True *β̂₁ ŷ ŷ x*β̂₁, &#160;&#160;&#160;bias=True&#45;&gt;ŷ indentity 실제로는 이 표현을 많이 사용함 | . (표현5) . - 벡터버전으로 표현하면 아래와 같다. 이 경우에는 ${ bf X}=[1,x]$에 포함된 1이 bias의 역할을 해주므로 bias = False 임. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@β̂, bias=False&quot;[label=&quot;@β̂&quot;] ; &quot;X@β̂, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@β̂, &#160;&#160;&#160;bias=False X@β̂, &#160;&#160;&#160;bias=False X&#45;&gt;X@β̂, &#160;&#160;&#160;bias=False @β̂ ŷ ŷ X@β̂, &#160;&#160;&#160;bias=False&#45;&gt;ŷ indentity 저는 이걸 좋아해요 | . (표현5)&#39; . - 딥러닝에서는 $ hat{ boldsymbol{ beta}}$ 대신에 $ hat$을 라고 표현한다. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@Ŵ, bias=False&quot;[label=&quot;@Ŵ&quot;] ; &quot;X@Ŵ, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@Ŵ, &#160;&#160;&#160;bias=False X@Ŵ, &#160;&#160;&#160;bias=False X&#45;&gt;X@Ŵ, &#160;&#160;&#160;bias=False @Ŵ ŷ ŷ X@Ŵ, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity - 실제로는 표현4 혹은 표현5를 외우면 된다. . Layer&#51032; &#44060;&#45392; . - (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다. . - 레이어는 항상 아래와 같은 규칙을 가진다. . 첫 동그라미는 레이어의 입력이다. | 첫번째 화살표는 선형변환을 의미한다. | 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) | 두번째 화살표는 두번째 동그라미에 어떠한 함수 $f$를 취하는 과정을 의미한다. (우리의 그림에서는 $f(x)=x$) | 세번째 동그라미는 레이어의 최종출력이다. | . - 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. . 레이어의 입력차원 | 선형변환의 결과로 얻어지는 차원 | 선형변환에서 바이어스를 쓸지? 안쓸지? | 함수 $f$ | - 주목: 1,2가 결정되면 자동으로 $ hat$의 차원이 결정된다. . (예시) . 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: $ hat{ bf W}$는 (2,1) 매트릭스 | 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: $ hat{ bf W}$는 (20,5) 매트릭스 | 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: $ hat{ bf W}$는 (2,50) 매트릭스 | . - 주목2: 이중에서 절대 생략불가능 것은 &quot;2. 선형변환의 결과로 얻어지는 차원&quot; 이다. . 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 $ hat{ bf W}$의 차원을 결정할 수 있음. | 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. | 함수 $f$: 기본적으로 항등함수를 가정하면 된다. | . Keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . - 기본뼈대: net생성 $ to$ add(layer) $ to$ compile(opt,loss) $ to$ fit(data,epochs) . - 데이터정리 . $${ bf y} approx 2.5 +4*x$$ . tnp.random.seed(43052) N= 200 x= tnp.linspace(0,1,N) epsilon= tnp.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1) # 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f109a893550&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)&gt;] . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . (0단계) 데이터정리 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f102c2b3b20&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#51104;&#49884;&#47928;&#48277;&#51221;&#47532; . - 잠깐 Dense layer를 만드는 코드를 정리해보자. . (1) 아래는 모두 같은 코드이다. . tf.keras.layers.Dense(1) | tf.keras.layers.Dense(units=1) | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;) // identity 가 더 맞는것 같은데.. | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;,use_bias=True) | . (2) 아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임) . tf.keras.layers.Dense(1,input_dim=2) # 코드1 | tf.keras.layers.Dense(1,input_shape=(2,)) # 코드2 | . (3) 아래는 사용불가능한 코드이다. . tf.keras.layers.Dense(1,input_dim=(2,)) # 코드1 | tf.keras.layers.Dense(1,input_shape=2) # 코드2 | . - 왜 input_dim이 필요한가? . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Dense(1,use_bias=False)) . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2)) . net1.weights . ValueError Traceback (most recent call last) Input In [36], in &lt;cell line: 1&gt;() -&gt; 1 net1.weights File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2542, in Model.weights(self) 2532 @property 2533 def weights(self): 2534 &#34;&#34;&#34;Returns the list of all layer variables/weights. 2535 2536 Note: This will not track the weights of nested `tf.Modules` that are not (...) 2540 A list of variables. 2541 &#34;&#34;&#34; -&gt; 2542 return self._dedup_weights(self._undeduplicated_weights) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2547, in Model._undeduplicated_weights(self) 2544 @property 2545 def _undeduplicated_weights(self): 2546 &#34;&#34;&#34;Returns the undeduplicated list of all layer variables/weights.&#34;&#34;&#34; -&gt; 2547 self._assert_weights_created() 2548 weights = [] 2549 for layer in self._self_tracked_trackables: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/sequential.py:471, in Sequential._assert_weights_created(self) 468 return 469 # When the graph has not been initialized, use the Model&#39;s implementation to 470 # to check if the weights has been created. --&gt; 471 super(functional.Functional, self)._assert_weights_created() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2736, in Model._assert_weights_created(self) 2728 return 2730 if (&#39;build&#39; in self.__class__.__dict__ and 2731 self.__class__ != Model and 2732 not self.built): 2733 # For any model that has customized build() method but hasn&#39;t 2734 # been invoked yet, this will cover both sequential and subclass model. 2735 # Also make sure to exclude Model class itself which has build() defined. -&gt; 2736 raise ValueError(f&#39;Weights for model {self.name} have not yet been &#39; 2737 &#39;created. &#39; 2738 &#39;Weights are created when the Model is first called on &#39; 2739 &#39;inputs or `build()` is called with an `input_shape`.&#39;) ValueError: Weights for model sequential_4 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`. . net2.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.4367702], [0.8878907]], dtype=float32)&gt;] . net1.summary() . ValueError Traceback (most recent call last) Input In [38], in &lt;cell line: 1&gt;() -&gt; 1 net1.summary() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2579, in Model.summary(self, line_length, positions, print_fn, expand_nested) 2559 &#34;&#34;&#34;Prints a string summary of the network. 2560 2561 Args: (...) 2576 ValueError: if `summary()` is called before the model is built. 2577 &#34;&#34;&#34; 2578 if not self.built: -&gt; 2579 raise ValueError( 2580 &#39;This model has not yet been built. &#39; 2581 &#39;Build the model first by calling `build()` or by calling &#39; 2582 &#39;the model on a batch of data.&#39;) 2583 layer_utils.print_summary( 2584 self, 2585 line_length=line_length, 2586 positions=positions, 2587 print_fn=print_fn, 2588 expand_nested=expand_nested) ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data. . net2.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_5 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . &#54400;&#51060;3: &#49828;&#52860;&#46972;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,input_dim=1) . net.add(layer) . . 초기값을 설정 . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[1.2078832]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . net.get_weights() . [array([[1.2078832]], dtype=float32), array([0.], dtype=float32)] . weight, bias순으로 출력 | . net.set_weights? . Signature: net.set_weights(weights) Docstring: Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer&#39;s weights must be instantiated before calling this function, by calling the layer. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) &gt;&gt;&gt; layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) &gt;&gt;&gt; layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights()) &gt;&gt;&gt; layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Args: weights: a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer&#39;s specifications. File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/base_layer.py Type: method . layer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군? | . - 한번따라해보자. . _w = net.get_weights() _w . [array([[1.2078832]], dtype=float32), array([0.], dtype=float32)] . 길이가 2인 리스트이고, 각 원소는 numpy array 임 | . net.set_weights( [np.array([[10.0]],dtype=np.float32), # weight, β1_hat np.array([-5.0],dtype=np.float32)] # bias, β0_hat ) . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) . (4단계) net.fit() . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f90b1d120&gt; . 결과확인 . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;] . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) . net.add(layer) . . 초기값을 설정하자 . net.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)]) . net.get_weights() . [array([[-5.], [10.]], dtype=float32)] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f7b9175e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_7/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] . - 사실 실전에서는 초기값을 설정할 필요가 별로 없음. . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204; &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) . net.add(layer) . (3단계) net.compile() . loss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N . net.compile(tf.keras.optimizers.SGD(0.1), loss_fn) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f914134f0&gt; . net.weights . [&lt;tf.Variable &#39;dense_8/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), loss=&#39;mse&#39;) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f7b9e02e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;7: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; + &#50741;&#54000;&#47560;&#51060;&#51200; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;) #net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32) #net.optimizer.lr = 0.1 . (4단계) net.fit() . net.fit(X,y,epochs=5000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f785482e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_22/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5848842], [3.9307659]], dtype=float32)&gt;] . &#50668;&#47084;&#44032;&#51648; &#54924;&#44480;&#47784;&#54805;&#51032; &#51201;&#54633;&#44284; &#54617;&#49845;&#44284;&#51221;&#51032; &#47784;&#45768;&#53552;&#47553; . &#50696;&#51228;1 . model: $y_i approx beta_0 + beta_1 x_i$ . np.random.seed(43052) N= 100 x= np.random.randn(N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X= np.stack([np.ones(N),x],axis=1) y= y.reshape(N,1) . plt.plot(x,y,&#39;o&#39;) # 관측한 자료 . [&lt;matplotlib.lines.Line2D at 0x7f0f7baa94e0&gt;] . beta_hat = np.array([-3,-2]).reshape(2,1) . yhat = X@beta_hat . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f7b9177c0&gt;] . 더 좋은 적합선을 얻기위해서! . slope = (2*X.T@X@beta_hat - 2*X.T@y)/ N beta_hat2 = beta_hat - 0.1*slope yhat2 = X@beta_hat2 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) plt.plot(x,yhat2.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f7a0b6230&gt;] . 초록색이 좀 더 나아보인다. . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) / N beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , 7.12238255, -1.2575366 , 5.73166742, -0.1555309 , 4.86767499, 0.51106397, 4.36611576, 0.87316777, 4.12348617, 1.01165173, 4.07771926, 0.97282343, 4.19586617, 0.77814101, 4.46653491, 0.4299822 , 4.89562729, -0.08537358, 5.50446319, -0.79684366, 6.32975688, -1.74933031, 7.42517729, -3.00603683, 8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129], [-2. , 8.70824998, 0.16165717, 6.93399596, 1.62435964, 5.72089586, 2.63858056, 4.86387722, 3.37280529, 4.22385379, 3.94259478, 3.70397678, 4.43004465, 3.23363047, 4.89701606, 2.75741782, 5.39439054, 2.22728903, 5.96886945, 1.59655409, 6.66836857, 0.81489407, 7.54676324, -0.17628423, 8.66856437, -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]]) . b0hats = beta_hats[0].tolist() b1hats = beta_hats[1].tolist() . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.5451404 ], [3.94818596]]) . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*x) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*x) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;2 . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f797c2020&gt;] . X= np.stack([np.ones(N),np.exp(-x)],axis=1) y= y.reshape(N,1) . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat.copy() # shallow copy, deep copy &lt; 여름 방학 특강 for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) /N beta_hat = beta_hat - 0.05*slope beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -1.74671631, -0.82428979, -0.14453919, 0.35720029, 0.72834869, 1.0036803 , 1.20869624, 1.36209751, 1.47759851, 1.56525696, 1.63244908, 1.68458472, 1.72563174, 1.75850062, 1.78532638, 1.80767543, 1.82669717, 1.84323521, 1.85790889, 1.8711731 , 1.88336212, 1.89472176, 1.90543297, 1.91562909, 1.92540859, 1.93484428, 1.94399023, 1.9528867 , 1.96156382], [-2. , -0.25663415, 1.01939241, 1.95275596, 2.63488171, 3.13281171, 3.49570765, 3.75961951, 3.95098231, 4.08918044, 4.18842797, 4.2591476 , 4.30898175, 4.34353413, 4.36691339, 4.38213187, 4.39139801, 4.39633075, 4.39811673, 4.3976256 , 4.3954946 , 4.3921905 , 4.38805511, 4.3833386 , 4.37822393, 4.37284482, 4.36729887, 4.36165718, 4.35597148, 4.35027923]]) . b0hats= beta_hats[0].tolist() b1hats= beta_hats[1].tolist() . np.linalg.inv(X.T@X)@X.T@y . array([[2.46307644], [3.99681332]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x)) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x)) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3 . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f6915f850&gt;] . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . beta_hat = np.array([-3,-2,-1]).reshape(3,1) beta_hats = beta_hat.copy() for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat -2*X.T@y) /N beta_hat = beta_hat - 0.1 * slope beta_hats= np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -0.71767532, 0.36255782, 0.89072137, 1.16423101, 1.31925078, 1.41819551, 1.48974454, 1.54713983, 1.59655416, 1.64091846, 1.68167278, 1.71956758, 1.75503084, 1.78833646, 1.81968188, 1.84922398, 1.877096 , 1.90341567, 1.92828934, 1.95181415, 1.97407943, 1.99516755, 2.01515463, 2.0341111 , 2.05210214, 2.06918818, 2.08542523, 2.10086524, 2.11555643], [-2. , 1.16947474, 2.64116513, 3.33411605, 3.66880042, 3.83768856, 3.92897389, 3.98315095, 4.01888831, 4.04486085, 4.06516144, 4.08177665, 4.09571971, 4.10754954, 4.1176088 , 4.12613352, 4.13330391, 4.13926816, 4.14415391, 4.14807403, 4.15112966, 4.1534121 , 4.15500404, 4.15598045, 4.15640936, 4.15635249, 4.15586584, 4.15500014, 4.15380139, 4.1523112 ], [-1. , -0.95492718, -0.66119313, -0.27681968, 0.12788212, 0.52254445, 0.89491388, 1.24088224, 1.55993978, 1.85310654, 2.12199631, 2.36839745, 2.59408948, 2.8007666 , 2.99000967, 3.16327964, 3.32192026, 3.46716468, 3.60014318, 3.72189116, 3.83335689, 3.93540864, 4.02884144, 4.11438316, 4.19270026, 4.26440288, 4.33004965, 4.39015202, 4.44517824, 4.49555703]]) . b0hats,b1hats,b2hats = beta_hats . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.46597526], [4.00095138], [5.04161877]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x)) # ax2: 오른쪽그림 # β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) # β0=β0.reshape(-1) # β1=β1.reshape(-1) # loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) # loss = list(map(loss_fn, β0,β1)) # ax2.scatter(β0,β1,loss,alpha=0.02) # ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x)) # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3: &#52992;&#46972;&#49828;&#47196; &#54644;&#48372;&#51088;! . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . net = tf.keras.Sequential() # 1: 네트워크 생성 net.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) # 3: compile net.fit(X,y,epochs=30, batch_size=N) # 4: fit . Epoch 1/30 1/1 [==============================] - 0s 60ms/step - loss: 47.7087 Epoch 2/30 1/1 [==============================] - 0s 1ms/step - loss: 21.4259 Epoch 3/30 1/1 [==============================] - 0s 1ms/step - loss: 14.1095 Epoch 4/30 1/1 [==============================] - 0s 1ms/step - loss: 11.0534 Epoch 5/30 1/1 [==============================] - 0s 1ms/step - loss: 9.1350 Epoch 6/30 1/1 [==============================] - 0s 915us/step - loss: 7.6614 Epoch 7/30 1/1 [==============================] - 0s 758us/step - loss: 6.4544 Epoch 8/30 1/1 [==============================] - 0s 733us/step - loss: 5.4484 Epoch 9/30 1/1 [==============================] - 0s 741us/step - loss: 4.6063 Epoch 10/30 1/1 [==============================] - 0s 768us/step - loss: 3.9007 Epoch 11/30 1/1 [==============================] - 0s 766us/step - loss: 3.3093 Epoch 12/30 1/1 [==============================] - 0s 745us/step - loss: 2.8135 Epoch 13/30 1/1 [==============================] - 0s 735us/step - loss: 2.3979 Epoch 14/30 1/1 [==============================] - 0s 876us/step - loss: 2.0495 Epoch 15/30 1/1 [==============================] - 0s 889us/step - loss: 1.7574 Epoch 16/30 1/1 [==============================] - 0s 886us/step - loss: 1.5126 Epoch 17/30 1/1 [==============================] - 0s 788us/step - loss: 1.3073 Epoch 18/30 1/1 [==============================] - 0s 850us/step - loss: 1.1352 Epoch 19/30 1/1 [==============================] - 0s 717us/step - loss: 0.9910 Epoch 20/30 1/1 [==============================] - 0s 678us/step - loss: 0.8700 Epoch 21/30 1/1 [==============================] - 0s 695us/step - loss: 0.7686 Epoch 22/30 1/1 [==============================] - 0s 746us/step - loss: 0.6836 Epoch 23/30 1/1 [==============================] - 0s 703us/step - loss: 0.6123 Epoch 24/30 1/1 [==============================] - 0s 710us/step - loss: 0.5526 Epoch 25/30 1/1 [==============================] - 0s 838us/step - loss: 0.5025 Epoch 26/30 1/1 [==============================] - 0s 935us/step - loss: 0.4605 Epoch 27/30 1/1 [==============================] - 0s 947us/step - loss: 0.4252 Epoch 28/30 1/1 [==============================] - 0s 824us/step - loss: 0.3957 Epoch 29/30 1/1 [==============================] - 0s 864us/step - loss: 0.3709 Epoch 30/30 1/1 [==============================] - 0s 831us/step - loss: 0.3501 . &lt;keras.callbacks.History at 0x7f0f68b8fbb0&gt; . net.weights . [&lt;tf.Variable &#39;dense_23/kernel:0&#39; shape=(3, 1) dtype=float32, numpy= array([[2.354784 ], [3.9989622], [4.58522 ]], dtype=float32)&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@net.weights).reshape(-1),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f095472a3b0&gt;] . &#49689;&#51228; . &#50696;&#51228;2: &#52992;&#46972;&#49828;&#47484; &#51060;&#50857;&#54616;&#50668; &#50500;&#47000;&#47484; &#47564;&#51313;&#54616;&#45716; &#51201;&#51208;&#54620; $ beta_0$&#50752; $ beta_1$&#51012; &#44396;&#54616;&#46972;. &#51201;&#54633;&#44208;&#44284;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. (&#50528;&#45768;&#47700;&#51060;&#49496; &#49884;&#44033;&#54868; X) . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon .",
            "url": "https://guebin.github.io/DS2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "relUrl": "/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "(5주차) 4월4일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . #!conda install -c conda-forge python-graphviz -y . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#54924;&#44480;&#48516;&#49437; &#47928;&#51228; . - ${ bf y} approx 2.5 + 4 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 y = 2.5+4*x + epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7eff782a2f50&gt;] . &#51060;&#47200;&#51201; &#54400;&#51060; . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . $S_{xx}=$, $S_{xy}=$ | $ hat{ beta}_0=$, $ hat{ beta}_1=$ | . - 풀이 . x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . Sxx=sum((x-x.mean())**2) Sxy=sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta0_hat = y.mean() - beta1_hat *x.mean() . beta0_hat, beta1_hat . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt;) . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . - 포인트 . $ hat{ beta}=(X&#39;X)^{-1}X&#39;y$ | . - 풀이 . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49552;&#49892;&#54632;&#49688;&#51032; &#46020;&#54632;&#49688;&#51060;&#50857; . - 포인트 . $loss&#39;( beta)=-2X&#39;y +2X&#39;X beta$ | $ beta_{new} = beta_{old} - alpha times loss&#39;( beta_{old})$ | . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tnp.array([-5,10]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[-5], [10]])&gt; . alpha=0.1 . $loss&#39;( beta) = -2X&#39;y+2X&#39;X beta$ . slope = -2*X.T @ y + 2*X.T@ X @ beta_hat slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1820.07378797], [ -705.77222696]])&gt; . step = - alpha*slope step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[182.0073788], [ 70.5772227]])&gt; . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N step = - alpha * slope beta_hat = beta_hat + step . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) plt.plot(x,X@beta_hat,&#39;b--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7eff78116950&gt;] . . GradientTape&#47484; &#51060;&#50857; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드1: 그레디언트 테입 with tf.GradientTape() as tape: loss = ## 포인트코드2: 미분 slope = tape.gradient(loss,beta_hat) ## 포인트코드3: update beta_hat.assign_sub(slope*alph) . - 풀이 . y=y.reshape(N,1) # N=200 X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: tape.watch(beta_hat) yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) beta_hat.assign_sub(slope * alpha) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 미분 slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) # N=200 x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat = beta0_hat + beta1_hat*x loss = tf.reduce_sum((y-yhat)**2) / N #loss = sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) beta0_hat.assign_sub(slope0 * alpha) beta1_hat.assign_sub(slope1 * alpha) . beta0_hat, beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.5836616&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.9330447&gt;) . GradientTape + opt.apply_gradients . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope,beta_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients( [(slope,beta_hat),(slope,beta_hat)] ) . (y-yhat).T @ (y-yhat) / N . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.25493942]])&gt; . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(-1) x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = beta0_hat + beta1_hat*x loss = tf.reduce_sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) opt.apply_gradients( [(slope0,beta0_hat),(slope1,beta1_hat)] ) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . . opt.minimize . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드1: 손실함수 정의 loss_fn = lambda: ?? ## 포인트코드2: 옵티마이저 생성 opt = tf.optimizers.SGD(alpha) ## 포인트코드3: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,beta_hat) . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . loss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat)/N . lambda x: x**2 &lt;=&gt; lambda(x) = x^2 | lambda x,y : x+y &lt;=&gt; lambda(x,y)=x+y | lambda: y &lt;=&gt; lambda() = y | . (y-X@beta_hat).T @ (y-X@beta_hat) . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[4811.45696758]])&gt; . loss_fn() . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[24.05728484]])&gt; . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . loss_fn = lambda: tf.reduce_sum((y-beta0_hat - beta1_hat*x)**2)/N . for epoc in range(1000): opt.minimize(loss_fn,[beta0_hat,beta1_hat]) # 미분 + update . beta0_hat, beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#51687;&#51008;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): return ?? . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): return (y-X@beta_hat).T @ (y-X@beta_hat)/N . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#44596;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): ?? ?? return ?? . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MSE . - 포인트 . ## 포인트코드: 미리구현되어있는 손실함수 이용 tf.losses.MSE(y,yhat) . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): yhat= X@beta_hat loss = tf.losses.MSE(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MeaSquaredError . - 포인트 . ## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . mseloss_fn = tf.losses.MeanSquaredError() . mseloss_fn(y.reshape(-1),yhat.reshape(-1)) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=24.05728530883789&gt; . def loss_fn(): yhat= X@beta_hat loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . tf.keras.Sequential . - $ hat{y}_i= hat{ beta}_0+ hat{ beta}_1x_i$ 의 서로다른 표현 . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta0_hat&quot;] &quot;x&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta1_hat&quot;] &quot;beta0_hat + x*beta1_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False 1&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta0_hat yhat yhat beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity x x x&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta1_hat gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*beta1_hat, bias=True&quot;[label=&quot;*beta1_hat&quot;] ; &quot;x*beta1_hat, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*beta1_hat, &#160;&#160;&#160;bias=True x*beta1_hat, &#160;&#160;&#160;bias=True x&#45;&gt;x*beta1_hat, &#160;&#160;&#160;bias=True *beta1_hat yhat yhat x*beta1_hat, &#160;&#160;&#160;bias=True&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;X=[1 x]&quot; -&gt; &quot;X@beta_hat, bias=False&quot;[label=&quot;@beta_hat&quot;] ; &quot;X@beta_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X=[1 x] X=[1 x] X@beta_hat, &#160;&#160;&#160;bias=False X@beta_hat, &#160;&#160;&#160;bias=False X=[1 x]&#45;&gt;X@beta_hat, &#160;&#160;&#160;bias=False @beta_hat yhat yhat X@beta_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드1: 네트워크 생성 net = tf.keras.Sequential() ## 포인트코드2: 네트워크의 아키텍처 설계 net.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) ## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=loss_fn2) ## 포인트코드4: 미분 &amp; update net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . - 풀이 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) # 아키텍처 설계 = yhat을 만들계획 . def loss_fn2(y,yhat): ## 손실함수의 정의 return (y-yhat).T @ (y-yhat) / N . alpha=0.1 . opt=tf.optimizers.SGD(alpha) ## 옵티마이저의 선택 . net.compile(opt,loss=loss_fn2) ## 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # 미분 &amp; update 의 반복 . &lt;keras.callbacks.History at 0x7f002cdba380&gt; . net.weights . [&lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] .",
            "url": "https://guebin.github.io/DS2022/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "relUrl": "/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "date": " • Apr 4, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "(5주차) 3월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#51032; &#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음 . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . alpha=0.01/6 . opt = tf.keras.optimizers.SGD(learning_rate=alpha) . opt.lr . 2022-04-04 09:23:41.660141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Variable &#39;learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt; . - opt에 전달할 입력값을 정리해보자 . beta= tf.Variable(-10.0) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-10.0&gt; . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . slope= tape.gradient(loss,beta) . - iter1: opt.apply_gradients() 에 값을 전달하여 beta를 1회 업데이트 . 주의점: opt.apply_gradients()의 입력으로 pair의 list를 전달해야함. | . opt.apply_gradients([(slope,beta)]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 slope= tape.gradient(loss,beta) . opt.apply_gradients([(slope,beta)]) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문을 이용한 반복 (정리) . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) for epoc in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 slope= tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . &#48169;&#48277;2: opt.minimize() . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) . loss_fn = lambda: (beta/2-1)**2 . - iter1 . opt.minimize(loss_fn,beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . opt.minimize(loss_fn,beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문을 구하는 코드로 정리 . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . - tf.keras.optimizers.SGD와 tf.optimizers.SGD의 차이? 없음 . (증거1) . _opt1=tf.keras.optimizers.SGD() . _opt2=tf.optimizers.SGD() . type(_opt1),type(_opt2) . (keras.optimizer_v2.gradient_descent.SGD, keras.optimizer_v2.gradient_descent.SGD) . 똑같다..? . (증거2) . alpha=0.01/6 opt = tf.optimizers.SGD(alpha) beta= tf.Variable(-10.0) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . (증거3) 모듈위치가 같다. . tf.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py&#39;&gt; File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py Docstring: Public API for tf.keras.optimizers namespace. . tf.keras.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py&#39;&gt; File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py Docstring: Public API for tf.keras.optimizers namespace. . &#54924;&#44480;&#48516;&#49437; . - ${ bf y} approx 4 + 2.5 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 . y= 2.5+4*x+epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7cc08ab30&gt;] . &#54400;&#51060;1 . Sxx = sum((x-x.mean())**2) Sxy = sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta0_hat = y.mean() - beta1_hat*x.mean() . beta0_hat,beta1_hat . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt;) . &#54400;&#51060;2 . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . tf.linalg.inv(X.T@X)@ X.T @y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta= tnp.array([-5.0,10.0]).reshape(2,1) . slope = -2*X.T@y + 2*X.T@X@beta . slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1820.07378797], [ -705.77222696]])&gt; . alpha = 0.001 . step = slope * alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1.82007379], [-0.70577223]])&gt; . &#49689;&#51228; . - 풀이3을 완성하라. 즉 경사하강법을 이용하여 적절한 beta를 추정하라. . iteration 횟수는 1000번으로 설정 | 학습률은 0.001로 설정 | beta의 초기값은 beta= tnp.array([-5.0,10.0]).reshape(2,1) | .",
            "url": "https://guebin.github.io/DS2022/2022/03/30/(5%EC%A3%BC%EC%B0%A8)-3%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2022/03/30/(5%EC%A3%BC%EC%B0%A8)-3%EC%9B%9430%EC%9D%BC.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "(4주차) 3월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. (우리는 답을 알고 있긴 함 $ beta=2$) . &#48169;&#48277;1: grid search . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 비판1: [-10,10]이외에 해가 존재하면? . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? | . - 비판2: 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | . &#48169;&#48277;2: gradient descent . &#50508;&#44256;&#47532;&#51608; . (1) 임의의 초기값을 선정하고 loss를 계산한다. . $ beta=-5 to loss(-5)=(-5/2-1)^2=12.25$ | . (-5/2-1)**2 . 12.25 . (2) 임의의 초기값에서 좌우로 약간씩 이동해보고 loss를 계산한다. . 왼쪽으로 이동: $ beta=-5.01, quad loss(5.01)=12.285025$ | 오른쪽으로 이동: $ beta=-4.99, quad loss(-4.99)=12.215025 $ | . (-5.01 /2 -1)**2 . 12.285025 . (-4.99 /2 -1)**2 . 12.215025 . (3) (2)의 결과를 보고 어느쪽으로 이동하는것이 유리한지 따져보고 유리한 방향으로 이동한다. . $ beta=-4.99$ 로 이동 | . (4) (2)-(3) 의 과정을 반복한다. 왼쪽/오른쪽 모두 가봐도 유리한 지점이 없다면 알고리즘을 멈춘다. . &#51104;&#44624; &#50508;&#44256;&#47532;&#51608; &#44048;&#49345; . - 알고리즘이 멈추는 지점은 $ beta=2$이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문. . - 이 알고리즘은 $loss=(x beta-y)^2$의 꼴에서 $[-10,10]$ 이외의 지점에 해가 존재하여도 적절하게 해를 찾을 것. . - 또한 비효율적으로 $ beta=2$ 이후에도 탐색을 반복하지 않는다. . - 알고리즘해석 . (2)의 의미: 미분을 하라는 뜻 . (3)의 의미: update . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . - 미분계수의 의미 . 미분계수가 양수이다: 왼쪽으로 이동 = 빼기 0.01 | 미분계수가 음수이다: 오른쪽으로 이동 = 더하기 0.01 | . - 수식화 . $$ beta_{next} = begin{cases} beta_{old} - 0.01 &amp; loss&#39;( beta_{old})&gt;0 beta_{old} + 0.01 &amp; loss&#39;( beta_{old})&lt;0 end{cases}$$ . &#54841;&#49884;, &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - 동일하게 0.01씩 이동하는게 맞을까? . _beta = np.linspace(-10,5) plt.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f396c6c43d0&gt;] . - 위의 그림에서 $ beta=-10$ 일 경우의 접선의 기울기는 $-6$이고 $ beta=-4$ 일때 접선의 기울기는 $-3$이다. . $ because loss = (0.5 beta-1)^2 to loss&#39; = 0.5 beta-1$ . $ beta=-10$에서 0.01만큼 이동했다면 $ beta=-4$에서 0.005만큼 이동해야함 | . - 아이디어를 수식화하자! . $$ beta_{next} leftarrow beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta= beta_{old}}$$ . 아까 수식이랑 좀 다르다? 달라보이지만 $ beta_{old}$를 이동시켜 $ beta_{next}$를 만든다는 개념은 같음 | $ alpha&gt;0$ | $ alpha$의 의미: 한번 업데이트할때 움직이는 보폭 | $ alpha= frac{0.01}{6}$ 로 만약 설정하면 $ beta=-10$일때 오른쪽으로 0.01움직임 | . (개선한 알고리즘을 이용한 풀이) . iter1: beta=-10 출발 . beta = tf.Variable(-10.0) . 2022-03-28 22:10:20.721327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . alpha=0.01/6 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . iter2 beta=-9.99 . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . 왜 tf.Variable의 메소드에 assign_add, assign_sub 정도만 있는지? | persistenct도 왜 디폴트가 False인지? | . for문 . (수업용) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.997125&gt; . (시도1) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . (시도2) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(1000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133687&gt; . &#54617;&#49845;&#47456; . - 목표: 아래의 학습과정을 시각화해보자. . beta = tf.Variable(-10.0) alpha=0.01/6 for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . [&#49884;&#44033;&#54868;&#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] . - 환경설정 . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; . from matplotlib import animation . - 도화지와 네모틀 생성 . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() # ax 네모틀 . fig . - 도화지와 네모틀는 포함관계에 있음. . fig.axes . [&lt;AxesSubplot:&gt;] . id(fig.axes[0]) . 139884400738752 . id(ax) . 139884400738752 . - 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함 . pnts, = ax.plot([1,2,3],[3,4,5],&#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7f39600862f0&gt; . fig . - pnts 오브젝트: x,y data를 변경해보자. . pnts.get_xdata() . array([1, 2, 3]) . pnts.get_ydata() . array([3, 4, 5]) . pnts.set_ydata([4,4,4]) . pnts.get_ydata() . [4, 4, 4] . fig . - 에니매이션 . def animate(i): if i%2 == 0: pnts.set_ydata([3,4,5]) else: pnts.set_ydata([4,4,4]) . ani=animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect 예비학습끝 . - 다시 학습과정 시각화 문제로 돌아오자. . beta_lst = [-10.0,-9.00,-8.00] loss_lst = [(-10.0/2-1)**2,(-9.00/2-1)**2,(-8.00/2-1)**2] . fig = plt.figure() ax = fig.add_subplot() . _beta= np.linspace(-15,19) ax.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f3950174040&gt;] . fig . pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani=animation.FuncAnimation(fig,animate,frames=3) ani . &lt;/input&gt; Once Loop Reflect - 학습과정을 beta_lst, loss_lst로 저장하자. . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=0.01/6 . beta.numpy() . -10.0 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha를 조정한다!! ($ alpha=0.1$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=0.1 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha를 더 크게하면? ($ alpha=1$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=1 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha는 학습속도를 의미함. $ to$ 빨리배우는게 좋으니까 학습률이 크면 무조건 좋은거아닌가? $ to$ 아니에용 . (예시1) 너무 큰 학습률의 비효율성 ($ alpha=3.9$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=3.9 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect (예시2) 너무 큰 학습률의 위험성 ($ alpha=4.05$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=4.05 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect &#49689;&#51228; . 경사하강법을 이용하여 $y=(x-1)^2$의 최소값을 구하고 이를 애니메이션으로 시각화하라. (100번정도에 수렴하도록 적당한 학습률을 설정할것) .",
            "url": "https://guebin.github.io/DS2022/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "(4주차) 3월23일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp . import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제9: 카페예제로 돌아오자. . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . tnp.random.seed(43052) y= 10.2+ x*2.2 + tnp.random.randn(10) y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 , 66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])&gt; . beta0= tf.Variable(9.0) beta1= tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape: loss=sum((y-beta0-beta1*x)**2) . tape.gradient(loss,beta0),tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-127.597534&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3214.2532&gt;) . - 예제10: 카페예제의 매트릭스 버전 . X= tnp.array([1]*10+ [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta_true = tnp.array([[10.2],[2.2]]) beta_true . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[10.2], [ 2.2]])&gt; . tnp.random.seed(43052) y= X@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[54.98269924], [60.27348365], [61.27621687], [60.53495888], [62.9770905 ], [66.32168996], [66.87781372], [71.0050025 ], [72.63837337], [77.11143943]])&gt; . beta = tnp.array([[9.0],[2.0]]) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat = X@beta loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 해석적풀이 . $$loss&#39;( beta)= -2X&#39;y + 2X&#39;X beta$$ . -2 * X.T @ y + 2* X.T @ X @ beta # 이론적인 값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ boldsymbol{ hat beta}$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 $ bf{0}$인지 확인하라. (단 ${ bf 0}$은 길이가 2이고 각 원소가 0인 벡터) . betahat = tf.linalg.inv(X.T @ X) @ X.T @ y betahat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . with tf.GradientTape() as tape: tape.watch(betahat) yhat = X@betahat loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,betahat) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-4.23483471e-12], [-1.06379688e-10]])&gt; . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. (답은 이미 알고 있어요, $ beta=2$입니다.) . &#48169;&#48277;1: grid search . &#50508;&#44256;&#47532;&#51608; . (1) beta = [-10, -9.99, -9.98, ... , 9.99, 10] 와 같은 수열을 만든다. . (2) 각 지점에서 (beta/2 -1)^2 을 계산한다. . (3) (2)의 결과를 가장 작게 만드는 값을 고른다. . &#44396;&#54788;&#53076;&#46300; . beta = tnp.linspace(-10,10,100) loss = (beta/2 -1)**2 . loss . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([3.60000000e+01, 3.47980818e+01, 3.36165697e+01, 3.24554637e+01, 3.13147638e+01, 3.01944700e+01, 2.90945822e+01, 2.80151005e+01, 2.69560249e+01, 2.59173554e+01, 2.48990919e+01, 2.39012346e+01, 2.29237833e+01, 2.19667381e+01, 2.10300990e+01, 2.01138659e+01, 1.92180390e+01, 1.83426181e+01, 1.74876033e+01, 1.66529946e+01, 1.58387920e+01, 1.50449954e+01, 1.42716049e+01, 1.35186205e+01, 1.27860422e+01, 1.20738700e+01, 1.13821039e+01, 1.07107438e+01, 1.00597898e+01, 9.42924191e+00, 8.81910009e+00, 8.22936435e+00, 7.66003469e+00, 7.11111111e+00, 6.58259361e+00, 6.07448220e+00, 5.58677686e+00, 5.11947760e+00, 4.67258443e+00, 4.24609734e+00, 3.84001632e+00, 3.45434139e+00, 3.08907254e+00, 2.74420977e+00, 2.41975309e+00, 2.11570248e+00, 1.83205795e+00, 1.56881951e+00, 1.32598714e+00, 1.10356086e+00, 9.01540659e-01, 7.19926538e-01, 5.58718498e-01, 4.17916539e-01, 2.97520661e-01, 1.97530864e-01, 1.17947148e-01, 5.87695133e-02, 1.99979594e-02, 1.63248648e-03, 3.67309458e-03, 2.61197837e-02, 6.89725538e-02, 1.32231405e-01, 2.15896337e-01, 3.19967350e-01, 4.44444444e-01, 5.89327620e-01, 7.54616876e-01, 9.40312213e-01, 1.14641363e+00, 1.37292113e+00, 1.61983471e+00, 1.88715437e+00, 2.17488011e+00, 2.48301194e+00, 2.81154984e+00, 3.16049383e+00, 3.52984389e+00, 3.91960004e+00, 4.32976227e+00, 4.76033058e+00, 5.21130497e+00, 5.68268544e+00, 6.17447199e+00, 6.68666463e+00, 7.21926334e+00, 7.77226814e+00, 8.34567901e+00, 8.93949597e+00, 9.55371901e+00, 1.01883481e+01, 1.08433833e+01, 1.15188246e+01, 1.22146720e+01, 1.29309254e+01, 1.36675849e+01, 1.44246505e+01, 1.52021222e+01, 1.60000000e+01])&gt; . (예비학습) . tnp.argmin([1,2,3,-1,5]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . tnp.argmin([1,2,-1,3,5]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . 예비학습 끝 . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . beta[59] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9191919191919187&gt; . beta[60] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.121212121212121&gt; . loss[59],loss[60] . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0016324864809713507&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0036730945821854847&gt;) . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 비판1: [-10,10]이외에 해가 존재하면? . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? | . - 비판2: 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | .",
            "url": "https://guebin.github.io/DS2022/2022/03/23/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9423%EC%9D%BC.html",
            "relUrl": "/2022/03/23/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9423%EC%9D%BC.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "(3주차) 3월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-03-21 13:59:52.470472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - 예제: (2,3,4), (2,3,4), (2,3,4) . (예시1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=-a c=2*a . a,b,c . (&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [16, 18, 20, 22]], [[24, 26, 28, 30], [32, 34, 36, 38], [40, 42, 44, 46]]], dtype=int32)&gt;) . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]], dtype=int32)&gt; . (예시4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) # axis=0 . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) # axis=1 . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) # axis=2 . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) # axis=3 . tf.stack([a,b,c],axis=3) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]], dtype=int32)&gt; . - 예제: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . tf.concat([a,b],axis=1) # 에러 . InvalidArgumentError Traceback (most recent call last) Input In [12], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) # 에러 . InvalidArgumentError Traceback (most recent call last) Input In [13], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . . tnp . - tf.constant 는 너무 쓰기 어렵다. . - 넘파이와 비교하여 생기는 불만 . (불만1) .reshape 메소드 불가능 // tf.reshape() 는 가능 . a=np.array([1,2,3,4]).reshape(2,2) a . array([[1, 2], [3, 4]]) . a=tf.constant([1,2,3,4]) tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . (불만2) 암묵적형변환(=알아서 눈치껏 형변환) 불가능 . np.array([1,2,3,4])+np.array([3.14,3.14,3.14,3.14]) . array([4.14, 5.14, 6.14, 7.14]) . (불만3) .transpose(), .T 불가능 // tf.transpose()는 가능 . np.array([1,2,3,4,5,6]).reshape(2,3).T . array([[1, 4], [2, 5], [3, 6]]) . tf.transpose(tf.reshape(tf.constant([1,2,3,4,5,6]), (2,3))) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 4], [2, 5], [3, 6]], dtype=int32)&gt; . (불만4) .max 불가능 // tf.reduce_max()는 가능 . np.array([1,2,3,4,5,6]).reshape(2,3).T.max() . 6 . tf.reduce_max(tf.transpose(tf.reshape(tf.constant([1,2,3,4,5,6]), (2,3)))) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt; . (불만?) (2,2) @ (2,) 의 연산 . numpy . np.array([[1.0,0.0],[0.0,1.0]]) @ np.array([77,-88]) . array([ 77., -88.]) . np.array([77,-88])@ np.array([[1.0,0.0],[0.0,1.0]]) . array([ 77., -88.]) . np.array([[1.0,0.0],[0.0,1.0]]) @ np.array([77,-88]).reshape(2,1) . array([[ 77.], [-88.]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1.0,0.0],[0.0,1.0]]) . ValueError Traceback (most recent call last) Input In [46], in &lt;cell line: 1&gt;() -&gt; 1 np.array([77,-88]).reshape(2,1) @ np.array([[1.0,0.0],[0.0,1.0]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . tf . I= tf.constant([[1.0,0.0],[0.0,1.0]]) x = tf.constant([77,-88]) . I@ x . InvalidArgumentError Traceback (most recent call last) Input In [49], in &lt;cell line: 1&gt;() -&gt; 1 I@ x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul] . x @ I . InvalidArgumentError Traceback (most recent call last) Input In [50], in &lt;cell line: 1&gt;() -&gt; 1 x @ I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:MatMul] . ... 저 이런거 하루종일 생각해낼수도 있어요 . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;) . import tensorflow.experimental.numpy as tnp # 앞으로 텐서플로에서 np 대신에 tnp를 사용하면 넘파이에 익숙한 문법을 모두 쓸 수 있고 tnp.experimental_enable_numpy_behavior() # 생성된 tf.constant 자료형은 넘파이와 유사하게 동작한다. . &#49440;&#50616;, &#49440;&#50616;&#44256;&#44553; . tnp.array([1,2,3]) . &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt; . tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy= array([[1, 0], [0, 1]])&gt; . &#53440;&#51077; . type(tf.constant([1,2,3])) . tensorflow.python.framework.ops.EagerTensor . type(tnp.diag([1,1])) . tensorflow.python.framework.ops.EagerTensor . tf.constant&#47196; &#47564;&#46308;&#44256; numpy&#52376;&#47100; &#50416;&#44592; . - reshape, transpose, T . tnp.array([1,2,3,4]).reshape(2,2) . &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy= array([[1, 2], [3, 4]])&gt; . tf.constant([1,2,3,4]).reshape(2,2) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.constant([1,2,3,4]).reshape(2,2).T . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . tf.constant([1,2,3,4]).reshape(2,2).transpose().T . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - max . tf.constant([1,2,3,4]).reshape(2,2).transpose().T.max() . &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt; . - 알아서 형 변환 . tf.constant([1,2,3]) + tf.constant([3.14,3.14,3.14]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([4.1400001, 5.1400001, 6.1400001])&gt; . - (2,2) @ (2,) 의 연산? . tnp.diag([1,1]) @ tf.constant([77,-88]) . &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 77, -88])&gt; . tf.constant([77,-88]) @ tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 77, -88])&gt; . tnp&#45716; &#44144;&#51032; np&#50752; &#50976;&#49324;&#54632; $ to$ &#54616;&#51648;&#47564; &#50756;&#51204;&#55176; &#44057;&#51008;&#44163; &#50500;&#45768;&#45796;. . a = np.array([1,2,3]) a . array([1, 2, 3]) . a[0]=11 a . array([11, 2, 3]) . a=tnp.array([1,2,3]) a . &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt; . a[0] . &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [84], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . tf.Variable . &#49440;&#50616; . - tf.Variable()로 선언 . a=tf.Variable([1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . - tf.constant() 선언후 변환 . tf.Variable(tf.constant([1,2,3])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . - np 등으로 선언후 변환 . tf.Variable(np.array([1,2,3])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=int64, numpy=array([1, 2, 3])&gt; . &#53440;&#51077; . a=tf.Variable([1,2]) type(a) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a= tf.Variable([1,2,3,4]) a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . &#50672;&#49328;&#44032;&#45733; . - 더하기 . tf.Variable([1,2])+tf.Variable([3,4]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . - 그런데 tf.Variable 끼리 연산해도 그 결과가 tf.Variable 인것은 아님. (왜 이렇게 만들었어??) . a=tf.Variable([1,2]) b=tf.Variable([3,4]) type(a),type(b) . (tensorflow.python.ops.resource_variable_ops.ResourceVariable, tensorflow.python.ops.resource_variable_ops.ResourceVariable) . type(a+b) . tensorflow.python.framework.ops.EagerTensor . tnp&#51032; &#51008;&#52509;&#51012; &#51068;&#48512;&#47564; &#48155;&#51020; . - 알아서 형 변환 . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape 메소드 . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [113], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060; &#50630;&#51020; . - tf.concat . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) . a= tf.Variable([1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . a.assign_add([-1,-2]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2,) dtype=int32, numpy=array([0, 0], dtype=int32)&gt; . &#50836;&#50557; . - tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음. . - 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. . &#48120;&#48516; . &#47784;&#54000;&#48652; . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . (손풀이) . $ frac{dy}{dx}=6x$ 이므로, $x=2$를 대입하면 답은 12이다. . (컴퓨터로풀이) . 도함수를 구하려니까 갑자기 어려움 . 그런데 $x=2$에서 접선의 기울기만 계산하려고 마음먹으면 쉬움 . 단계1: 답만계산 . x1=2 y1=3*x1**2 . x2=2+0.000001 y2=3*x2**2 . (y2-y1)/(x2-x1) . 12.000003000266702 . 단계2: 함수화 . def f(x): return 3*x**2 . def d(f,x): return (f(x+0.000001)-f(x))/0.000001 . d(f,2) . 12.000003001944037 . 단계3: lambda . d(lambda x: x**2,3) . 6.000001000927568 . - 2개의 변수로 가지는 함수를 만들어보자. . def f(x,y): return x**2 + 3*y . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [153], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [149], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000001)-f(x))/0.000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . - 아쉽지만 손으로 함수를 직접 구현하여 미분을 하나하나 계산하기에는 한계가 있겠음 . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제1: $x=2$에서 $y=3x^2$의 도함수값을 구하라. . x=tf.Variable(2.0) a=tf.constant(3.0) . mytape=tf.GradientTape() . mytape.__enter__() # 테이프를 기록 y=a*x**2 # y=ax^2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제2: 조금 다른예제 . x=tf.Variable(2.0) #a=tf.constant(3.0) mytape=tf.GradientTape() mytape.__enter__() # 테이프를 기록 a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.__exit__(None,None,None) mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $a= frac{3}{2}x$ . $y=ax^2= frac{3}{2}x^3$ . $ frac{dy}{dx}= frac{3}{2}3 x^2$ . 1.5 * 3 * 4 . 18.0 . - 테이프의 개념 ($ star$) . (상황) . 우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 $y=3x^2$) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함. . (1) mytape = tf.GradientTape(): 컴퓨터에게 전달할 공책가 만들어짐, 연습장의 이름을 mytape이라고 쓴다. . (2) mytape.__enter__(): mytape공책을 연다 . (3) a=x/2*3; y=a*x**2: 컴퓨터에게 부탁할 수식을 쓴다. . (4) mytape.__exit__(None,None,None): mytape라는 공책을 닫는다. . (5) mytape.gradient(y,x): $y$를 $x$로 미분한다는 포스트잇을 남겨서 컴퓨터한테 전달 . - 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다. . x=tf.Variable(2.0) a=x/2*3 # a=x*(3/2) mytape=tf.GradientTape() mytape.__enter__() # 테이프를 기록 #a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.__exit__(None,None,None) mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제4: with문과 함께 쓰는 tf.GradientTape() . x=tf.Variable(2.0) a=x/2*3 # a=x*(3/2) with tf.GradientTape() as mytape: y=a*x**2 # y=ax^2=(3/2)x^3 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . (해설) . with문은 아래와 같이 동작한다. . with expression as myname: ### with문 시작 blabla~! yadiyadi~ ### with문 끝 . (1) expression이 실행되면서 오브젝트가 하나 생성됨, 그 오브젝트를 myname이라고 받음 . (2) with문이 시작되면서 myname.__enter__() 가 실행 . (3) 블라블라, 야디야디 실행 . (4) with문이 끝나면서 myname.__exit__() 이 실행 . - 예제5: 예제2를 with문과 함께 구현 . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제6: persistent = True . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) # 2번실행해서 에러를 관찰하자. . RuntimeError Traceback (most recent call last) Input In [182], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1032, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 1002 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1003 1004 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1029 called with an unknown value. 1030 &#34;&#34;&#34; 1031 if self._tape is None: -&gt; 1032 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1033 &#34;compute one set of gradients (or jacobians)&#34;) 1034 if self._recording: 1035 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) # 2번실행해서 에러를 관찰하자. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제7: watch . (관찰1) 미분하라는게 없으면 아무것도 출력안함 . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제8: 자동감시기능 off . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape(watch_accessed_variables=False) as mytape: #mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(watch_accessed_variables=False) as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰3) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . &#49689;&#51228; . $y=x^2$에서 $x=0$에서의 접선의 기울기를 구하라. (tf.GradientTape()를 이용할것) .",
            "url": "https://guebin.github.io/DS2022/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "(3주차) 3월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#49440;&#50616;&#44256;&#44553; . - 대각행렬선언 . tf.constant(np.diag([1,2,3])) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . - 1만 포함한 텐서를 만들고 싶음 . tf.ones([3,4]) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.reshape(tf.constant([1]*12),(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], dtype=int32)&gt; . - 0만 포함한 텐서를 만들고 싶음 . tf.zeros((3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . - 배열선언 . tf.constant(range(12)) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.constant(range(2,6)) # 2,3,4,5 . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 3, 4, 5], dtype=int32)&gt; . tf.constant(range(2,21,3)) . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 2, 5, 8, 11, 14, 17, 20], dtype=int32)&gt; . tf.linspace(0,1,14) . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ])&gt; . tf.linspace(-1,20,14) . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ])&gt; . tf.linspace([0,-1],[1,20],14,axis=0) # axis=0은 생략가능 . &lt;tf.Tensor: shape=(14, 2), dtype=float64, numpy= array([[ 0. , -1. ], [ 0.07692308, 0.61538462], [ 0.15384615, 2.23076923], [ 0.23076923, 3.84615385], [ 0.30769231, 5.46153846], [ 0.38461538, 7.07692308], [ 0.46153846, 8.69230769], [ 0.53846154, 10.30769231], [ 0.61538462, 11.92307692], [ 0.69230769, 13.53846154], [ 0.76923077, 15.15384615], [ 0.84615385, 16.76923077], [ 0.92307692, 18.38461538], [ 1. , 20. ]])&gt; . tf.linspace([0,-1],[1,20],14,axis=1) . &lt;tf.Tensor: shape=(2, 14), dtype=float64, numpy= array([[ 0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ], [-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ]])&gt; . - 랜덤 . tf.random.normal([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[-1.5421661 , -1.5808113 , 1.1388296 ], [-2.6040976 , 1.0960393 , 0.02348358], [-0.53655666, 1.2877383 , -0.17504291]], dtype=float32)&gt; . tf.random.normal([10]) . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([ 0.32824913, 0.01634766, 1.157158 , 0.60476935, -0.03189921, 1.0994794 , 0.7413038 , -0.6990549 , -1.6825281 , -1.2637503 ], dtype=float32)&gt; . tf.random.uniform([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0.04361665, 0.17482626, 0.50426793], [0.71013105, 0.6988857 , 0.49747908], [0.74961853, 0.6538025 , 0.09697211]], dtype=float32)&gt; . np.random.randn(10) . array([-0.53429023, 1.13505005, -1.43100272, -2.13399699, -1.05582216, -1.61879362, -0.11808444, 0.87534624, 2.64117769, -0.8208995 ]) . tf.random.randn(10) . AttributeError Traceback (most recent call last) Input In [39], in &lt;cell line: 1&gt;() -&gt; 1 tf.random.randn(10) AttributeError: module &#39;tensorflow._api.v2.random&#39; has no attribute &#39;randn&#39; . tf.concat . - (2,1) concat (2,1) =&gt; (2,2) . 두번쨰축이 1에서 2로 바뀌네? // axis=1 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . - (2,1) concat (2,1) =&gt; (4,1) . 첫번쨰 축이 바뀜 // axis=0 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (2,2) ` . 첫번쨰축// axis=0 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (1,4) ` . 두번쨰축// axis=1 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (4,2,3) . 첫번쨰축 // axis=0 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (2,4,3) . 두번쨰축 // axis=1 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (2,2,6) . 세번쨰축 // axis=2, axis = -1 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 2, 6), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2], [ 3, 4, 5, -3, -4, -5]], [[ 6, 7, 8, -6, -7, -8], [ 9, 10, 11, -9, -10, -11]]], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (8,) . 첫번쨰 축 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (4,2) . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [78], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.concat은 차원을 증가시키면서 결합해주는것은 아니다. | . tf.stack . - (4) stack (4) =&gt; (4,2) . (4,) stack (4,) =&gt; (4,2) // 두번째 축이 비어있다고 인식 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . - (4) stack (4) =&gt; (2,4) . (,4) stack (,4) =&gt; (2,4) // 첫번째 축이 비어있다고 인식 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,2,3,4,5) . 첫번째축이 비어있다고 인식 | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,2,3,4,5) . 두번째축이 비어있다고 인식 | (2, ,3,4,5) stack (2, ,3,4,5) | (2,2,3,4,5) | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,3,4,5,2) . 마지막축이 비어있다고 인식 | (2,3,4,5,) stack (2,3,4,5,) | (2,3,4,5,2) | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a c=-a*2 . 아래의 코드에서 ??를 어떻게 바꿔야?? . tf.stack([a,b],axis=??) ## 숙제 .",
            "url": "https://guebin.github.io/DS2022/2022/03/16/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2022/03/16/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9416%EC%9D%BC.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "(2주차) 3월14일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) # check GPU . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - (2,) vector . lst = [1,2] . lst . [1, 2] . lst[0] # . 1 . lst[1] . 2 . - (2,2) matrix . lst = [[1,2],[3,4]] . lst[0][0] . 1 . lst[1][1] . 4 . 이것은 아래와 같은 매트릭스처럼 생각할 수 있음 . 1 2 3 4 . print(lst[0][0]) # (1,1) print(lst[0][1]) # (1,2) print(lst[1][0]) # (2,1) print(lst[1][1]) # (2,2) . 1 2 3 4 . 매트릭스는 아니지만 매트릭스 같음 . - (4,1) matrix . lst = [[1,2],[3,4],[5,6],[7,8]] lst # (4,2) . [[1, 2], [3, 4], [5, 6], [7, 8]] . lst = [[1],[3],[5],[7]] lst # (4,1) matrix = 길이가 4인 col-vector . [[1], [3], [5], [7]] . - (1,4) matrix . lst = [[1,2,3,4],[5,6,7,8]] lst # (2,4) matrix . [[1, 2, 3, 4], [5, 6, 7, 8]] . lst = [[1,2,3,4]] lst # (1,4) matrix = 길이가 4인 row-vector . [[1, 2, 3, 4]] . - 3차원 . lst = [[[1,2],[3,4]], [[5,6],[7,8]]] lst . [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] . print(lst[0][0][0]) print(lst[0][0][1]) print(lst[0][1][0]) print(lst[0][1][1]) print(lst[1][0][0]) print(lst[1][0][1]) print(lst[1][1][0]) print(lst[1][1][1]) . 1 2 3 4 5 6 7 8 . &#49440;&#50616; . - 스칼라 . _scalar =tf.constant(1) _scalar . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . - 벡터 . _vector=tf.constant([1,2,3]) _vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _vector[-2] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt; . - 매트릭스 . _matrix=tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0,1] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt; . &#53440;&#51077; . type(_scalar) . tensorflow.python.framework.ops.EagerTensor . &#51064;&#45937;&#49905; . _matrix=tf.constant([[1,2,3],[3,4,5]]) _matrix . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [3, 4, 5]], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _matrix[:,0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt; . _matrix[:,-1] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 5], dtype=int32)&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 불편한점 . dtype이 모든 원소가 똑같아야한다. (그럴수 있음) | 값을 바꿀 수가 없다. | dtype이 다르면 연산이 불가능함. | - 값을 바꿀 수 없다. . _matrix=tf.constant([[1,2,3],[3,4,5]]) _matrix . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [3, 4, 5]], dtype=int32)&gt; . _matrix[0,0]=22 . TypeError Traceback (most recent call last) Input In [138], in &lt;cell line: 1&gt;() -&gt; 1 _matrix[0,0]=22 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - dtype이 다르면 연산이 불가능함 . tf.constant([1.1,2])+tf.constant([3,4]) . InvalidArgumentError Traceback (most recent call last) Input In [139], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+tf.constant([3,4]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] . tf.constant([1.1,2])+tf.constant([3.0,4.0]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.1, 6. ], dtype=float32)&gt; . - 같은 float형이라도 불가능할 때가 있음 . tf.constant([1.1,2]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1, 2. ], dtype=float32)&gt; . tf.constant([3.0,4.0],dtype=tf.float64) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3., 4.])&gt; . tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) . InvalidArgumentError Traceback (most recent call last) Input In [143], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . _vector = tf.constant([1,2,3,4]) _vector . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . np.array(_vector) # 방법1 . array([1, 2, 3, 4], dtype=int32) . _vector.numpy() . array([1, 2, 3, 4], dtype=int32) . &#50672;&#49328; . - 더하기 . a=tf.constant([1,2,3]) b=tf.constant([4,5,6]) a+b . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)&gt; . - 곱하기 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[4,5],[6,7]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 4, 10], [18, 28]], dtype=int32)&gt; . a,b . (&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[4, 5], [6, 7]], dtype=int32)&gt;) . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 4, 10], [18, 28]], dtype=int32)&gt; . - 행렬곱 . a=tf.constant([[1,0],[0,1]]) b= tf.constant([[1],[22]]) . a @ b # (2,2) matrix @ (2,1) matrix . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[ 1], [22]], dtype=int32)&gt; . b @ a # (2,1) matrix @ (2,2) matrix . InvalidArgumentError Traceback (most recent call last) Input In [154], in &lt;cell line: 1&gt;() -&gt; 1 b @ a File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,1], In[1]: [2,2] [Op:MatMul] . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[ 1], [22]], dtype=int32)&gt; . - 역행렬 . a= tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) Input In [157], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1506, in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a= tf.constant([[1.0,2.0],[3.0,4.0]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 2.], [3., 4.]], dtype=float32)&gt; . tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-2. , 1. ], [ 1.5, -0.5]], dtype=float32)&gt; . a @ tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . - tf.linalg. + tab을 누르면 쓸만한게 조금 나온다. . a=tf.constant([[1.0,2.0],[3.0,4.0]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 2.], [3., 4.]], dtype=float32)&gt; . tf.linalg.det(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . - 단순한 집계함수들 . a=tf.constant([[1,2,3],[4,5,6]]) a . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]], dtype=int32)&gt; . tf.reduce_sum(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt; . tf.reduce_max(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt; . - 행렬곱 고급 . _I=tf.constant([[1.0,0.0],[0.0,1.0]]) _I . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . _x = tf.constant([22.0,33.0]) _x . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([22., 33.], dtype=float32)&gt; . (상황1) . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [169], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . _x @ _I . InvalidArgumentError Traceback (most recent call last) Input In [170], in &lt;cell line: 1&gt;() -&gt; 1 _x @ _I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . (상황2) . _x = tf.constant([[1.0],[2.0]]) _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[1.], [2.]], dtype=float32)&gt; . _I @ _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[1.], [2.]], dtype=float32)&gt; . _x @ _I . InvalidArgumentError Traceback (most recent call last) Input In [173], in &lt;cell line: 1&gt;() -&gt; 1 _x @ _I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,1], In[1]: [2,2] [Op:MatMul] . (상황3) . _x = tf.constant([[1.0,2.0]]) _x . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 2.]], dtype=float32)&gt; . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [175], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,2], In[1]: [1,2] [Op:MatMul] . _x @ _I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 2.]], dtype=float32)&gt; . (넘파이는 다르다..) . np.array([22,33]) @ np.diag([1,1]) # (2,) @ (2,2) ==&gt; (2,)를 (1,2)벡터로 해석해줘서 곱하기 수행, 수행결과는 다시 (2,)로 저장 . array([22, 33]) . np.array([[22],[33]]) @ np.diag([1,1]) # (2,1) @ (2,2) . ValueError Traceback (most recent call last) Input In [181], in &lt;cell line: 1&gt;() -&gt; 1 np.array([[22],[33]]) @ np.diag([1,1]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . &#54805;&#53468;&#48320;&#54872; . - tf.reshape 기본 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.reshape(a,(4,1)) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . tf.reshape(a,(1,4)) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . - tf.reshape 응용 . a= tf.constant([0,1,2,3,4,5,6,7,8,9,10,11]) # length 12 vector a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . tf.reshape(a,(3,-1)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 대각행렬선언 . tf.constant(np.diag([1,2,3])) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . - 1만 포함한 텐서를 만들고 싶음 . tf.ones([3,4]) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.reshape(tf.constant([1]*12),(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], dtype=int32)&gt; .",
            "url": "https://guebin.github.io/DS2022/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "(1주차) 3월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . &#44053;&#51032;&#48372;&#52649;&#51088;&#47308; . - https://github.com/guebin/DS2022/blob/master/_notebooks/2022-03-07-supp1.pdf . - https://github.com/guebin/DS2022/blob/master/_notebooks/2022-03-07-supp2.pdf . &#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: (선형대수학 $ to$) 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 나는 동네에 커피점을 하나 차렸음. | 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. | 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 &#39;온도 -&gt; 아이스아메리카노 판매량 예측&#39; 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) | . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . 온도 ${ bf x}$가 아래와 같다고 하자. . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . [&lt;matplotlib.lines.Line2D at 0x7f114c62d7e0&gt;] . - 우리의 목표: 파란색점 $ to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $ beta_0, beta_1$) 가 존재할것 같다. . $y_{i} approx ax_{i}+b$ | $y_{i} approx beta_1 x_{i}+ beta_0$ | . - 어림짐작으로 $a,b$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $a=2, b=15$ 혹은 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521086&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . - 풀어보자. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.84898&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;) # 초록색선: ture, 세상의법칙 . [&lt;matplotlib.lines.Line2D at 0x7f1128716d70&gt;] . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. . - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . - 모형의 매트릭스화 . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면: $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ &lt;-- 외우세요 . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X=tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945175 , 2.2156773], dtype=float32)&gt; . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573243234018&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.215704607783491&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457324, 2.21570461])&gt; . &#50526;&#51004;&#47196; &#54624;&#44163; . - 선형대수학의 미분이론.. . - 실습 (tensorflow에서 매트릭스를 자유롭게 다루비) .",
            "url": "https://guebin.github.io/DS2022/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "date": " • Mar 7, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널 1 | . 2022년 1학기 종료후 폐쇄예정 &#8617; . |",
          "url": "https://guebin.github.io/DS2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/DS2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}