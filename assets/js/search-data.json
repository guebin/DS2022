{
  
    
        "post0": {
            "title": "(5주차) 4월4일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#54924;&#44480;&#48516;&#49437; &#47928;&#51228; . - ${ bf y} approx 4 + 2.5 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 y = 2.5+4*x + epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7eff782a2f50&gt;] . &#51060;&#47200;&#51201; &#54400;&#51060; . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . $S_{xx}=$, $S_{xy}=$ | $ hat{ beta}_0=$, $ hat{ beta}_1=$ | . x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . Sxx=sum((x-x.mean())**2) Sxy=sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta0_hat = y.mean() - beta1_hat *x.mean() . beta0_hat, beta1_hat . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt;) . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . - 포인트 . $ hat{ beta}=(X&#39;X)^{-1}X&#39;y$ | . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49552;&#49892;&#54632;&#49688;&#51032; &#46020;&#54632;&#49688;&#51060;&#50857; . - 포인트 . $loss&#39;( beta)=-2X&#39;y +2X&#39;X beta$ | $ beta_{new} = beta_{old} - alpha times loss&#39;( beta_{old})$ | . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tnp.array([-5,10]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[-5], [10]])&gt; . alpha=0.1 . $loss&#39;( beta) = -2X&#39;y+2X&#39;X beta$ . slope = -2*X.T @ y + 2*X.T@ X @ beta_hat slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1820.07378797], [ -705.77222696]])&gt; . step = - alpha*slope step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[182.0073788], [ 70.5772227]])&gt; . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T@ X @ beta_hat)/N step = - alpha * slope beta_hat = beta_hat + step . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) plt.plot(x,X@beta_hat,&#39;b--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7eff78116950&gt;] . . GradientTape&#47484; &#51060;&#50857; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드1: 그레디언트 테입 with tf.GradientTape() as tape: loss = ## 포인트코드2: 미분 slope = tape.gradient(loss,beta_hat) ## 포인트코드3: update beta_hat.assign_sub(slope*alph) . y=y.reshape(N,1) # N=200 X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: tape.watch(beta_hat) yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) beta_hat.assign_sub(slope * alpha) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 미분 slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) . y=y.reshape(-1) # N=200 x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat = beta0_hat + beta1_hat*x loss = tf.reduce_sum((y-yhat)**2) / N #loss = sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) beta0_hat.assign_sub(slope0 * alpha) beta1_hat.assign_sub(slope1 * alpha) . beta0_hat, beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.5836616&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.9330447&gt;) . GradientTape + opt.apply_gradients . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope,beta_hat)]) ## pair의 list가 입력 . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients( [(slope,beta_hat),(slope,beta_hat)] ) . (y-yhat).T @ (y-yhat) / N . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.25493942]])&gt; . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 . y=y.reshape(-1) x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat = beta0_hat + beta1_hat*x loss = tf.reduce_sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) opt.apply_gradients( [(slope0,beta0_hat),(slope1,beta1_hat)] ) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . . opt.minimize . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드1: 손실함수 정의 loss_fn = lambda: ?? ## 포인트코드2: 옵티마이저 생성 opt = tf.optimizers.SGD(alpha) ## 포인트코드3: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,beta_hat) . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . loss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat)/N . lambda x: x**2 &lt;=&gt; lambda(x) = x^2 | lambda x,y : x+y &lt;=&gt; lambda(x,y)=x+y | lambda: y &lt;=&gt; lambda() = y | . (y-X@beta_hat).T @ (y-X@beta_hat) . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[4811.45696758]])&gt; . loss_fn() . &lt;tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[24.05728484]])&gt; . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . y=y.reshape(-1) x.shape,y.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . loss_fn = lambda: tf.reduce_sum((y-beta0_hat - beta1_hat*x)**2)/N . for epoc in range(1000): opt.minimize(loss_fn,[beta0_hat,beta1_hat]) # 미분 + update . beta0_hat, beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#51687;&#51008;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): return ?? . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): return (y-X@beta_hat).T @ (y-X@beta_hat)/N . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#44596;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): ?? ?? return ?? . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): yhat = X@beta_hat loss = (y-yhat).T @ (y-yhat) / N return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MSE . - 포인트 . ## 포인트코드: 미리구현되어있는 손실함수 이용 tf.losses.MSE(y,yhat) . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . def loss_fn(): yhat= X@beta_hat loss = tf.losses.MSE(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MeaSquaredError . - 포인트 . ## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . opt = tf.optimizers.SGD(alpha) . mseloss_fn = tf.losses.MeanSquaredError() . mseloss_fn(y.reshape(-1),yhat.reshape(-1)) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=24.05728530883789&gt; . def loss_fn(): yhat= X@beta_hat loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) return loss . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) # 미분 + update . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . tf.keras.Sequential . - $ hat{y}_i= hat{ beta}_0+ hat{ beta}_1x_i$ 의 서로다른 표현 . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta0_hat&quot;] &quot;x&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta1_hat&quot;] &quot;beta0_hat + x*beta1_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False 1&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta0_hat yhat yhat beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity x x x&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta1_hat gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*beta1_hat, bias=True&quot;[label=&quot;*beta1_hat&quot;] ; &quot;x*beta1_hat, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*beta1_hat, &#160;&#160;&#160;bias=True x*beta1_hat, &#160;&#160;&#160;bias=True x&#45;&gt;x*beta1_hat, &#160;&#160;&#160;bias=True *beta1_hat yhat yhat x*beta1_hat, &#160;&#160;&#160;bias=True&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;X=[1 x]&quot; -&gt; &quot;X@beta_hat, bias=False&quot;[label=&quot;@beta_hat&quot;] ; &quot;X@beta_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X=[1 x] X=[1 x] X@beta_hat, &#160;&#160;&#160;bias=False X@beta_hat, &#160;&#160;&#160;bias=False X=[1 x]&#45;&gt;X@beta_hat, &#160;&#160;&#160;bias=False @beta_hat yhat yhat X@beta_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드1: 네트워크 생성 net = tf.keras.Sequential() ## 포인트코드2: 네트워크의 아키텍처 설계 net.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) ## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=loss_fn2) ## 포인트코드4: 미분 &amp; update net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . y=y.reshape(N,1) X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) # 아키텍처 설계 = yhat을 만들계획 . def loss_fn2(y,yhat): ## 손실함수의 정의 return (y-yhat).T @ (y-yhat) / N . alpha=0.1 . opt=tf.optimizers.SGD(alpha) ## 옵티마이저의 선택 . net.compile(opt,loss=loss_fn2) ## 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # 미분 &amp; update 의 반복 . &lt;keras.callbacks.History at 0x7effdb6b75b0&gt; . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . - 포인트 . ## 포인트코드: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=&#39;mse&#39;) # net.compile의 옵션으로 손실함수 지정 . &#54400;&#51060;3: &#49828;&#52860;&#46972;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . - 포인트 . ## 포인트코드: use_bias=True net.add(tf.keras.layers.Dense(1,input_shape=(1,),use_bias=True)) . tf.keras.Model . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 네트워크 설계 (Xi -&gt; yihat의 과정을 공책에 쭉 쓰는 느낌) Xi = tf.keras.layers.Input(shape=(2,)) l1 = tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False) yihat = l1(Xi) net = tf.keras.Model(Xi,yihat) . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 네트워크 설계 (입력차원 조심, 바이어스 조심) xi = tf.keras.layers.Input(shape=(1,)) l1 = tf.keras.layers.Dense(units=1,input_shape=(1,),use_bias=True) .",
            "url": "https://guebin.github.io/DS2022/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "relUrl": "/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "date": " • Apr 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(5주차) 3월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#51032; &#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음 . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . alpha=0.01/6 . opt = tf.keras.optimizers.SGD(learning_rate=alpha) . opt.lr . 2022-04-04 09:23:41.660141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;tf.Variable &#39;learning_rate:0&#39; shape=() dtype=float32, numpy=0.0016666667&gt; . - opt에 전달할 입력값을 정리해보자 . beta= tf.Variable(-10.0) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-10.0&gt; . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . slope= tape.gradient(loss,beta) . - iter1: opt.apply_gradients() 에 값을 전달하여 beta를 1회 업데이트 . 주의점: opt.apply_gradients()의 입력으로 pair의 list를 전달해야함. | . opt.apply_gradients([(slope,beta)]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 slope= tape.gradient(loss,beta) . opt.apply_gradients([(slope,beta)]) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문을 이용한 반복 (정리) . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) for epoc in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 slope= tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . &#48169;&#48277;2: opt.minimize() . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) . loss_fn = lambda: (beta/2-1)**2 . - iter1 . opt.minimize(loss_fn,beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . opt.minimize(loss_fn,beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문을 구하는 코드로 정리 . alpha=0.01/6 opt = tf.keras.optimizers.SGD(alpha) beta= tf.Variable(-10.0) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . - tf.keras.optimizers.SGD와 tf.optimizers.SGD의 차이? 없음 . (증거1) . _opt1=tf.keras.optimizers.SGD() . _opt2=tf.optimizers.SGD() . type(_opt1),type(_opt2) . (keras.optimizer_v2.gradient_descent.SGD, keras.optimizer_v2.gradient_descent.SGD) . 똑같다..? . (증거2) . alpha=0.01/6 opt = tf.optimizers.SGD(alpha) beta= tf.Variable(-10.0) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . (증거3) 모듈위치가 같다. . tf.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py&#39;&gt; File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py Docstring: Public API for tf.keras.optimizers namespace. . tf.keras.optimizers? . Type: module String form: &lt;module &#39;keras.api._v2.keras.optimizers&#39; from &#39;/home/cgb3/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py&#39;&gt; File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/api/_v2/keras/optimizers/__init__.py Docstring: Public API for tf.keras.optimizers namespace. . &#54924;&#44480;&#48516;&#49437; . - ${ bf y} approx 4 + 2.5 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 . y= 2.5+4*x+epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fc7cc08ab30&gt;] . &#54400;&#51060;1 . Sxx = sum((x-x.mean())**2) Sxy = sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta0_hat = y.mean() - beta1_hat*x.mean() . beta0_hat,beta1_hat . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt;) . &#54400;&#51060;2 . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) y=y.reshape(N,1) . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . tf.linalg.inv(X.T@X)@ X.T @y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . beta= tnp.array([-5.0,10.0]).reshape(2,1) . slope = -2*X.T@y + 2*X.T@X@beta . slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1820.07378797], [ -705.77222696]])&gt; . alpha = 0.001 . step = slope * alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-1.82007379], [-0.70577223]])&gt; . &#49689;&#51228; . - 풀이3을 완성하라. 즉 경사하강법을 이용하여 적절한 beta를 추정하라. . iteration 횟수는 1000번으로 설정 | 학습률은 0.001로 설정 | beta의 초기값은 beta= tnp.array([-5.0,10.0]).reshape(2,1) | .",
            "url": "https://guebin.github.io/DS2022/2022/03/30/(5%EC%A3%BC%EC%B0%A8)-3%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2022/03/30/(5%EC%A3%BC%EC%B0%A8)-3%EC%9B%9430%EC%9D%BC.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(4주차) 3월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. (우리는 답을 알고 있긴 함 $ beta=2$) . &#48169;&#48277;1: grid search . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 비판1: [-10,10]이외에 해가 존재하면? . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? | . - 비판2: 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | . &#48169;&#48277;2: gradient descent . &#50508;&#44256;&#47532;&#51608; . (1) 임의의 초기값을 선정하고 loss를 계산한다. . $ beta=-5 to loss(-5)=(-5/2-1)^2=12.25$ | . (-5/2-1)**2 . 12.25 . (2) 임의의 초기값에서 좌우로 약간씩 이동해보고 loss를 계산한다. . 왼쪽으로 이동: $ beta=-5.01, quad loss(5.01)=12.285025$ | 오른쪽으로 이동: $ beta=-4.99, quad loss(-4.99)=12.215025 $ | . (-5.01 /2 -1)**2 . 12.285025 . (-4.99 /2 -1)**2 . 12.215025 . (3) (2)의 결과를 보고 어느쪽으로 이동하는것이 유리한지 따져보고 유리한 방향으로 이동한다. . $ beta=-4.99$ 로 이동 | . (4) (2)-(3) 의 과정을 반복한다. 왼쪽/오른쪽 모두 가봐도 유리한 지점이 없다면 알고리즘을 멈춘다. . &#51104;&#44624; &#50508;&#44256;&#47532;&#51608; &#44048;&#49345; . - 알고리즘이 멈추는 지점은 $ beta=2$이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문. . - 이 알고리즘은 $loss=(x beta-y)^2$의 꼴에서 $[-10,10]$ 이외의 지점에 해가 존재하여도 적절하게 해를 찾을 것. . - 또한 비효율적으로 $ beta=2$ 이후에도 탐색을 반복하지 않는다. . - 알고리즘해석 . (2)의 의미: 미분을 하라는 뜻 . (3)의 의미: update . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . - 미분계수의 의미 . 미분계수가 양수이다: 왼쪽으로 이동 = 빼기 0.01 | 미분계수가 음수이다: 오른쪽으로 이동 = 더하기 0.01 | . - 수식화 . $$ beta_{next} = begin{cases} beta_{old} - 0.01 &amp; loss&#39;( beta_{old})&gt;0 beta_{old} + 0.01 &amp; loss&#39;( beta_{old})&lt;0 end{cases}$$ . &#54841;&#49884;, &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - 동일하게 0.01씩 이동하는게 맞을까? . _beta = np.linspace(-10,5) plt.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f396c6c43d0&gt;] . - 위의 그림에서 $ beta=-10$ 일 경우의 접선의 기울기는 $-6$이고 $ beta=-4$ 일때 접선의 기울기는 $-3$이다. . $ because loss = (0.5 beta-1)^2 to loss&#39; = 0.5 beta-1$ . $ beta=-10$에서 0.01만큼 이동했다면 $ beta=-4$에서 0.005만큼 이동해야함 | . - 아이디어를 수식화하자! . $$ beta_{next} leftarrow beta_{old} - alpha left[ frac{ partial}{ partial beta} loss( beta) right]_{ beta= beta_{old}}$$ . 아까 수식이랑 좀 다르다? 달라보이지만 $ beta_{old}$를 이동시켜 $ beta_{next}$를 만든다는 개념은 같음 | $ alpha&gt;0$ | $ alpha$의 의미: 한번 업데이트할때 움직이는 보폭 | $ alpha= frac{0.01}{6}$ 로 만약 설정하면 $ beta=-10$일때 오른쪽으로 0.01움직임 | . (개선한 알고리즘을 이용한 풀이) . iter1: beta=-10 출발 . beta = tf.Variable(-10.0) . 2022-03-28 22:10:20.721327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . alpha=0.01/6 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . iter2 beta=-9.99 . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . beta.assign_sub(alpha*tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . 왜 tf.Variable의 메소드에 assign_add, assign_sub 정도만 있는지? | persistenct도 왜 디폴트가 False인지? | . for문 . (수업용) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.997125&gt; . (시도1) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . (시도2) . beta = tf.Variable(-10.0) alpha=0.01/6 . for k in range(1000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133687&gt; . &#54617;&#49845;&#47456; . - 목표: 아래의 학습과정을 시각화해보자. . beta = tf.Variable(-10.0) alpha=0.01/6 for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . [&#49884;&#44033;&#54868;&#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] . - 환경설정 . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; . from matplotlib import animation . - 도화지와 네모틀 생성 . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() # ax 네모틀 . fig . - 도화지와 네모틀는 포함관계에 있음. . fig.axes . [&lt;AxesSubplot:&gt;] . id(fig.axes[0]) . 139884400738752 . id(ax) . 139884400738752 . - 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함 . pnts, = ax.plot([1,2,3],[3,4,5],&#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7f39600862f0&gt; . fig . - pnts 오브젝트: x,y data를 변경해보자. . pnts.get_xdata() . array([1, 2, 3]) . pnts.get_ydata() . array([3, 4, 5]) . pnts.set_ydata([4,4,4]) . pnts.get_ydata() . [4, 4, 4] . fig . - 에니매이션 . def animate(i): if i%2 == 0: pnts.set_ydata([3,4,5]) else: pnts.set_ydata([4,4,4]) . ani=animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect 예비학습끝 . - 다시 학습과정 시각화 문제로 돌아오자. . beta_lst = [-10.0,-9.00,-8.00] loss_lst = [(-10.0/2-1)**2,(-9.00/2-1)**2,(-8.00/2-1)**2] . fig = plt.figure() ax = fig.add_subplot() . _beta= np.linspace(-15,19) ax.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7f3950174040&gt;] . fig . pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani=animation.FuncAnimation(fig,animate,frames=3) ani . &lt;/input&gt; Once Loop Reflect - 학습과정을 beta_lst, loss_lst로 저장하자. . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=0.01/6 . beta.numpy() . -10.0 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha를 조정한다!! ($ alpha=0.1$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=0.1 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha를 더 크게하면? ($ alpha=1$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=1 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect - alpha는 학습속도를 의미함. $ to$ 빨리배우는게 좋으니까 학습률이 크면 무조건 좋은거아닌가? $ to$ 아니에용 . (예시1) 너무 큰 학습률의 비효율성 ($ alpha=3.9$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=3.9 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect (예시2) 너무 큰 학습률의 위험성 ($ alpha=4.05$) . beta_lst = [] loss_lst = [] . beta = tf.Variable(-10.0) alpha=4.05 . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(alpha*tape.gradient(loss,beta)) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() # fig 는 도화지 . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani=animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect &#49689;&#51228; . 경사하강법을 이용하여 $y=(x-1)^2$의 최소값을 구하고 이를 애니메이션으로 시각화하라. (100번정도에 수렴하도록 적당한 학습률을 설정할것) .",
            "url": "https://guebin.github.io/DS2022/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(4주차) 3월23일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp . import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제9: 카페예제로 돌아오자. . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) x . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])&gt; . tnp.random.seed(43052) y= 10.2+ x*2.2 + tnp.random.randn(10) y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 , 66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])&gt; . beta0= tf.Variable(9.0) beta1= tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape: loss=sum((y-beta0-beta1*x)**2) . tape.gradient(loss,beta0),tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-127.597534&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3214.2532&gt;) . - 예제10: 카페예제의 매트릭스 버전 . X= tnp.array([1]*10+ [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta_true = tnp.array([[10.2],[2.2]]) beta_true . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[10.2], [ 2.2]])&gt; . tnp.random.seed(43052) y= X@beta_true + tnp.random.randn(10).reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[54.98269924], [60.27348365], [61.27621687], [60.53495888], [62.9770905 ], [66.32168996], [66.87781372], [71.0050025 ], [72.63837337], [77.11143943]])&gt; . beta = tnp.array([[9.0],[2.0]]) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat = X@beta loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,beta) # 텐서플로우가 계산한 미분값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 해석적풀이 . $$loss&#39;( beta)= -2X&#39;y + 2X&#39;X beta$$ . -2 * X.T @ y + 2* X.T @ X @ beta # 이론적인 값 . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -127.59753624], [-3214.25306574]])&gt; . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ boldsymbol{ hat beta}$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 $ bf{0}$인지 확인하라. (단 ${ bf 0}$은 길이가 2이고 각 원소가 0인 벡터) . betahat = tf.linalg.inv(X.T @ X) @ X.T @ y betahat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[12.10040012], [ 2.13112662]])&gt; . with tf.GradientTape() as tape: tape.watch(betahat) yhat = X@betahat loss=(y-yhat).T @ (y-yhat) . tape.gradient(loss,betahat) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-4.23483471e-12], [-1.06379688e-10]])&gt; . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. (답은 이미 알고 있어요, $ beta=2$입니다.) . &#48169;&#48277;1: grid search . &#50508;&#44256;&#47532;&#51608; . (1) beta = [-10, -9.99, -9.98, ... , 9.99, 10] 와 같은 수열을 만든다. . (2) 각 지점에서 (beta/2 -1)^2 을 계산한다. . (3) (2)의 결과를 가장 작게 만드는 값을 고른다. . &#44396;&#54788;&#53076;&#46300; . beta = tnp.linspace(-10,10,100) loss = (beta/2 -1)**2 . loss . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([3.60000000e+01, 3.47980818e+01, 3.36165697e+01, 3.24554637e+01, 3.13147638e+01, 3.01944700e+01, 2.90945822e+01, 2.80151005e+01, 2.69560249e+01, 2.59173554e+01, 2.48990919e+01, 2.39012346e+01, 2.29237833e+01, 2.19667381e+01, 2.10300990e+01, 2.01138659e+01, 1.92180390e+01, 1.83426181e+01, 1.74876033e+01, 1.66529946e+01, 1.58387920e+01, 1.50449954e+01, 1.42716049e+01, 1.35186205e+01, 1.27860422e+01, 1.20738700e+01, 1.13821039e+01, 1.07107438e+01, 1.00597898e+01, 9.42924191e+00, 8.81910009e+00, 8.22936435e+00, 7.66003469e+00, 7.11111111e+00, 6.58259361e+00, 6.07448220e+00, 5.58677686e+00, 5.11947760e+00, 4.67258443e+00, 4.24609734e+00, 3.84001632e+00, 3.45434139e+00, 3.08907254e+00, 2.74420977e+00, 2.41975309e+00, 2.11570248e+00, 1.83205795e+00, 1.56881951e+00, 1.32598714e+00, 1.10356086e+00, 9.01540659e-01, 7.19926538e-01, 5.58718498e-01, 4.17916539e-01, 2.97520661e-01, 1.97530864e-01, 1.17947148e-01, 5.87695133e-02, 1.99979594e-02, 1.63248648e-03, 3.67309458e-03, 2.61197837e-02, 6.89725538e-02, 1.32231405e-01, 2.15896337e-01, 3.19967350e-01, 4.44444444e-01, 5.89327620e-01, 7.54616876e-01, 9.40312213e-01, 1.14641363e+00, 1.37292113e+00, 1.61983471e+00, 1.88715437e+00, 2.17488011e+00, 2.48301194e+00, 2.81154984e+00, 3.16049383e+00, 3.52984389e+00, 3.91960004e+00, 4.32976227e+00, 4.76033058e+00, 5.21130497e+00, 5.68268544e+00, 6.17447199e+00, 6.68666463e+00, 7.21926334e+00, 7.77226814e+00, 8.34567901e+00, 8.93949597e+00, 9.55371901e+00, 1.01883481e+01, 1.08433833e+01, 1.15188246e+01, 1.22146720e+01, 1.29309254e+01, 1.36675849e+01, 1.44246505e+01, 1.52021222e+01, 1.60000000e+01])&gt; . (예비학습) . tnp.argmin([1,2,3,-1,5]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . tnp.argmin([1,2,-1,3,5]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . 예비학습 끝 . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . beta[59] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.9191919191919187&gt; . beta[60] . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.121212121212121&gt; . loss[59],loss[60] . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0016324864809713507&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.0036730945821854847&gt;) . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 비판1: [-10,10]이외에 해가 존재하면? . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? | . - 비판2: 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | .",
            "url": "https://guebin.github.io/DS2022/2022/03/23/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9423%EC%9D%BC.html",
            "relUrl": "/2022/03/23/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9423%EC%9D%BC.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(3주차) 3월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . 2022-03-21 13:59:52.470472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - 예제: (2,3,4), (2,3,4), (2,3,4) . (예시1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=-a c=2*a . a,b,c . (&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [16, 18, 20, 22]], [[24, 26, 28, 30], [32, 34, 36, 38], [40, 42, 44, 46]]], dtype=int32)&gt;) . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]], dtype=int32)&gt; . (예시4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) # axis=0 . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) # axis=1 . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) # axis=2 . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) # axis=3 . tf.stack([a,b,c],axis=3) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]], dtype=int32)&gt; . - 예제: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . tf.concat([a,b],axis=1) # 에러 . InvalidArgumentError Traceback (most recent call last) Input In [12], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) # 에러 . InvalidArgumentError Traceback (most recent call last) Input In [13], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . . tnp . - tf.constant 는 너무 쓰기 어렵다. . - 넘파이와 비교하여 생기는 불만 . (불만1) .reshape 메소드 불가능 // tf.reshape() 는 가능 . a=np.array([1,2,3,4]).reshape(2,2) a . array([[1, 2], [3, 4]]) . a=tf.constant([1,2,3,4]) tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . (불만2) 암묵적형변환(=알아서 눈치껏 형변환) 불가능 . np.array([1,2,3,4])+np.array([3.14,3.14,3.14,3.14]) . array([4.14, 5.14, 6.14, 7.14]) . (불만3) .transpose(), .T 불가능 // tf.transpose()는 가능 . np.array([1,2,3,4,5,6]).reshape(2,3).T . array([[1, 4], [2, 5], [3, 6]]) . tf.transpose(tf.reshape(tf.constant([1,2,3,4,5,6]), (2,3))) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 4], [2, 5], [3, 6]], dtype=int32)&gt; . (불만4) .max 불가능 // tf.reduce_max()는 가능 . np.array([1,2,3,4,5,6]).reshape(2,3).T.max() . 6 . tf.reduce_max(tf.transpose(tf.reshape(tf.constant([1,2,3,4,5,6]), (2,3)))) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt; . (불만?) (2,2) @ (2,) 의 연산 . numpy . np.array([[1.0,0.0],[0.0,1.0]]) @ np.array([77,-88]) . array([ 77., -88.]) . np.array([77,-88])@ np.array([[1.0,0.0],[0.0,1.0]]) . array([ 77., -88.]) . np.array([[1.0,0.0],[0.0,1.0]]) @ np.array([77,-88]).reshape(2,1) . array([[ 77.], [-88.]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1.0,0.0],[0.0,1.0]]) . ValueError Traceback (most recent call last) Input In [46], in &lt;cell line: 1&gt;() -&gt; 1 np.array([77,-88]).reshape(2,1) @ np.array([[1.0,0.0],[0.0,1.0]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . tf . I= tf.constant([[1.0,0.0],[0.0,1.0]]) x = tf.constant([77,-88]) . I@ x . InvalidArgumentError Traceback (most recent call last) Input In [49], in &lt;cell line: 1&gt;() -&gt; 1 I@ x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul] . x @ I . InvalidArgumentError Traceback (most recent call last) Input In [50], in &lt;cell line: 1&gt;() -&gt; 1 x @ I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute MatMul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:MatMul] . ... 저 이런거 하루종일 생각해낼수도 있어요 . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;) . import tensorflow.experimental.numpy as tnp # 앞으로 텐서플로에서 np 대신에 tnp를 사용하면 넘파이에 익숙한 문법을 모두 쓸 수 있고 tnp.experimental_enable_numpy_behavior() # 생성된 tf.constant 자료형은 넘파이와 유사하게 동작한다. . &#49440;&#50616;, &#49440;&#50616;&#44256;&#44553; . tnp.array([1,2,3]) . &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt; . tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy= array([[1, 0], [0, 1]])&gt; . &#53440;&#51077; . type(tf.constant([1,2,3])) . tensorflow.python.framework.ops.EagerTensor . type(tnp.diag([1,1])) . tensorflow.python.framework.ops.EagerTensor . tf.constant&#47196; &#47564;&#46308;&#44256; numpy&#52376;&#47100; &#50416;&#44592; . - reshape, transpose, T . tnp.array([1,2,3,4]).reshape(2,2) . &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy= array([[1, 2], [3, 4]])&gt; . tf.constant([1,2,3,4]).reshape(2,2) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.constant([1,2,3,4]).reshape(2,2).T . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . tf.constant([1,2,3,4]).reshape(2,2).transpose().T . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - max . tf.constant([1,2,3,4]).reshape(2,2).transpose().T.max() . &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt; . - 알아서 형 변환 . tf.constant([1,2,3]) + tf.constant([3.14,3.14,3.14]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([4.1400001, 5.1400001, 6.1400001])&gt; . - (2,2) @ (2,) 의 연산? . tnp.diag([1,1]) @ tf.constant([77,-88]) . &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 77, -88])&gt; . tf.constant([77,-88]) @ tnp.diag([1,1]) . &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 77, -88])&gt; . tnp&#45716; &#44144;&#51032; np&#50752; &#50976;&#49324;&#54632; $ to$ &#54616;&#51648;&#47564; &#50756;&#51204;&#55176; &#44057;&#51008;&#44163; &#50500;&#45768;&#45796;. . a = np.array([1,2,3]) a . array([1, 2, 3]) . a[0]=11 a . array([11, 2, 3]) . a=tnp.array([1,2,3]) a . &lt;tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3])&gt; . a[0] . &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [84], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . tf.Variable . &#49440;&#50616; . - tf.Variable()로 선언 . a=tf.Variable([1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . - tf.constant() 선언후 변환 . tf.Variable(tf.constant([1,2,3])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . - np 등으로 선언후 변환 . tf.Variable(np.array([1,2,3])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(3,) dtype=int64, numpy=array([1, 2, 3])&gt; . &#53440;&#51077; . a=tf.Variable([1,2]) type(a) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a= tf.Variable([1,2,3,4]) a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . &#50672;&#49328;&#44032;&#45733; . - 더하기 . tf.Variable([1,2])+tf.Variable([3,4]) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . - 그런데 tf.Variable 끼리 연산해도 그 결과가 tf.Variable 인것은 아님. (왜 이렇게 만들었어??) . a=tf.Variable([1,2]) b=tf.Variable([3,4]) type(a),type(b) . (tensorflow.python.ops.resource_variable_ops.ResourceVariable, tensorflow.python.ops.resource_variable_ops.ResourceVariable) . type(a+b) . tensorflow.python.framework.ops.EagerTensor . tnp&#51032; &#51008;&#52509;&#51012; &#51068;&#48512;&#47564; &#48155;&#51020; . - 알아서 형 변환 . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape 메소드 . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [113], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060; &#50630;&#51020; . - tf.concat . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) . a= tf.Variable([1,2]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . a.assign_add([-1,-2]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(2,) dtype=int32, numpy=array([0, 0], dtype=int32)&gt; . &#50836;&#50557; . - tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음. . - 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. . &#48120;&#48516; . &#47784;&#54000;&#48652; . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . (손풀이) . $ frac{dy}{dx}=6x$ 이므로, $x=2$를 대입하면 답은 12이다. . (컴퓨터로풀이) . 도함수를 구하려니까 갑자기 어려움 . 그런데 $x=2$에서 접선의 기울기만 계산하려고 마음먹으면 쉬움 . 단계1: 답만계산 . x1=2 y1=3*x1**2 . x2=2+0.000001 y2=3*x2**2 . (y2-y1)/(x2-x1) . 12.000003000266702 . 단계2: 함수화 . def f(x): return 3*x**2 . def d(f,x): return (f(x+0.000001)-f(x))/0.000001 . d(f,2) . 12.000003001944037 . 단계3: lambda . d(lambda x: x**2,3) . 6.000001000927568 . - 2개의 변수로 가지는 함수를 만들어보자. . def f(x,y): return x**2 + 3*y . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [153], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [149], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000001)-f(x))/0.000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . - 아쉽지만 손으로 함수를 직접 구현하여 미분을 하나하나 계산하기에는 한계가 있겠음 . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제1: $x=2$에서 $y=3x^2$의 도함수값을 구하라. . x=tf.Variable(2.0) a=tf.constant(3.0) . mytape=tf.GradientTape() . mytape.__enter__() # 테이프를 기록 y=a*x**2 # y=ax^2 mytape.__exit__(None,None,None) . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제2: 조금 다른예제 . x=tf.Variable(2.0) #a=tf.constant(3.0) mytape=tf.GradientTape() mytape.__enter__() # 테이프를 기록 a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.__exit__(None,None,None) mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $a= frac{3}{2}x$ . $y=ax^2= frac{3}{2}x^3$ . $ frac{dy}{dx}= frac{3}{2}3 x^2$ . 1.5 * 3 * 4 . 18.0 . - 테이프의 개념 ($ star$) . (상황) . 우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 $y=3x^2$) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함. . (1) mytape = tf.GradientTape(): 컴퓨터에게 전달할 공책가 만들어짐, 연습장의 이름을 mytape이라고 쓴다. . (2) mytape.__enter__(): mytape공책을 연다 . (3) a=x/2*3; y=a*x**2: 컴퓨터에게 부탁할 수식을 쓴다. . (4) mytape.__exit__(None,None,None): mytape라는 공책을 닫는다. . (5) mytape.gradient(y,x): $y$를 $x$로 미분한다는 포스트잇을 남겨서 컴퓨터한테 전달 . - 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다. . x=tf.Variable(2.0) a=x/2*3 # a=x*(3/2) mytape=tf.GradientTape() mytape.__enter__() # 테이프를 기록 #a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.__exit__(None,None,None) mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제4: with문과 함께 쓰는 tf.GradientTape() . x=tf.Variable(2.0) a=x/2*3 # a=x*(3/2) with tf.GradientTape() as mytape: y=a*x**2 # y=ax^2=(3/2)x^3 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . (해설) . with문은 아래와 같이 동작한다. . with expression as myname: ### with문 시작 blabla~! yadiyadi~ ### with문 끝 . (1) expression이 실행되면서 오브젝트가 하나 생성됨, 그 오브젝트를 myname이라고 받음 . (2) with문이 시작되면서 myname.__enter__() 가 실행 . (3) 블라블라, 야디야디 실행 . (4) with문이 끝나면서 myname.__exit__() 이 실행 . - 예제5: 예제2를 with문과 함께 구현 . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제6: persistent = True . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) # 2번실행해서 에러를 관찰하자. . RuntimeError Traceback (most recent call last) Input In [182], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1032, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 1002 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1003 1004 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1029 called with an unknown value. 1030 &#34;&#34;&#34; 1031 if self._tape is None: -&gt; 1032 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1033 &#34;compute one set of gradients (or jacobians)&#34;) 1034 if self._recording: 1035 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) # 2번실행해서 에러를 관찰하자. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제7: watch . (관찰1) 미분하라는게 없으면 아무것도 출력안함 . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제8: 자동감시기능 off . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape(watch_accessed_variables=False) as mytape: #mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(watch_accessed_variables=False) as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰3) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: mytape.watch(x) a=x/2*3 # a=x*(3/2) y=a*x**2 # y=ax^2=(3/2)x^3 . mytape.gradient(y,x) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . &#49689;&#51228; . $y=x^2$에서 $x=0$에서의 접선의 기울기를 구하라. (tf.GradientTape()를 이용할것) .",
            "url": "https://guebin.github.io/DS2022/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(3주차) 3월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#49440;&#50616;&#44256;&#44553; . - 대각행렬선언 . tf.constant(np.diag([1,2,3])) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . - 1만 포함한 텐서를 만들고 싶음 . tf.ones([3,4]) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.reshape(tf.constant([1]*12),(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], dtype=int32)&gt; . - 0만 포함한 텐서를 만들고 싶음 . tf.zeros((3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . - 배열선언 . tf.constant(range(12)) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.constant(range(2,6)) # 2,3,4,5 . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 3, 4, 5], dtype=int32)&gt; . tf.constant(range(2,21,3)) . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 2, 5, 8, 11, 14, 17, 20], dtype=int32)&gt; . tf.linspace(0,1,14) . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ])&gt; . tf.linspace(-1,20,14) . &lt;tf.Tensor: shape=(14,), dtype=float64, numpy= array([-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ])&gt; . tf.linspace([0,-1],[1,20],14,axis=0) # axis=0은 생략가능 . &lt;tf.Tensor: shape=(14, 2), dtype=float64, numpy= array([[ 0. , -1. ], [ 0.07692308, 0.61538462], [ 0.15384615, 2.23076923], [ 0.23076923, 3.84615385], [ 0.30769231, 5.46153846], [ 0.38461538, 7.07692308], [ 0.46153846, 8.69230769], [ 0.53846154, 10.30769231], [ 0.61538462, 11.92307692], [ 0.69230769, 13.53846154], [ 0.76923077, 15.15384615], [ 0.84615385, 16.76923077], [ 0.92307692, 18.38461538], [ 1. , 20. ]])&gt; . tf.linspace([0,-1],[1,20],14,axis=1) . &lt;tf.Tensor: shape=(2, 14), dtype=float64, numpy= array([[ 0. , 0.07692308, 0.15384615, 0.23076923, 0.30769231, 0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769, 0.76923077, 0.84615385, 0.92307692, 1. ], [-1. , 0.61538462, 2.23076923, 3.84615385, 5.46153846, 7.07692308, 8.69230769, 10.30769231, 11.92307692, 13.53846154, 15.15384615, 16.76923077, 18.38461538, 20. ]])&gt; . - 랜덤 . tf.random.normal([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[-1.5421661 , -1.5808113 , 1.1388296 ], [-2.6040976 , 1.0960393 , 0.02348358], [-0.53655666, 1.2877383 , -0.17504291]], dtype=float32)&gt; . tf.random.normal([10]) . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([ 0.32824913, 0.01634766, 1.157158 , 0.60476935, -0.03189921, 1.0994794 , 0.7413038 , -0.6990549 , -1.6825281 , -1.2637503 ], dtype=float32)&gt; . tf.random.uniform([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0.04361665, 0.17482626, 0.50426793], [0.71013105, 0.6988857 , 0.49747908], [0.74961853, 0.6538025 , 0.09697211]], dtype=float32)&gt; . np.random.randn(10) . array([-0.53429023, 1.13505005, -1.43100272, -2.13399699, -1.05582216, -1.61879362, -0.11808444, 0.87534624, 2.64117769, -0.8208995 ]) . tf.random.randn(10) . AttributeError Traceback (most recent call last) Input In [39], in &lt;cell line: 1&gt;() -&gt; 1 tf.random.randn(10) AttributeError: module &#39;tensorflow._api.v2.random&#39; has no attribute &#39;randn&#39; . tf.concat . - (2,1) concat (2,1) =&gt; (2,2) . 두번쨰축이 1에서 2로 바뀌네? // axis=1 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . - (2,1) concat (2,1) =&gt; (4,1) . 첫번쨰 축이 바뀜 // axis=0 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (2,2) ` . 첫번쨰축// axis=0 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (1,4) ` . 두번쨰축// axis=1 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (4,2,3) . 첫번쨰축 // axis=0 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (2,4,3) . 두번쨰축 // axis=1 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5], [ 0, -1, -2], [ -3, -4, -5]], [[ 6, 7, 8], [ 9, 10, 11], [ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt; . - (2,2,3) concat (2,2,3) =&gt; (2,2,6) . 세번쨰축 // axis=2, axis = -1 | . a=tf.reshape(tf.constant(range(12)),(2,2,3)) b=-a a,b . (&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, -1, -2], [ -3, -4, -5]], [[ -6, -7, -8], [ -9, -10, -11]]], dtype=int32)&gt;) . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 2, 6), dtype=int32, numpy= array([[[ 0, 1, 2, 0, -1, -2], [ 3, 4, 5, -3, -4, -5]], [[ 6, 7, 8, -6, -7, -8], [ 9, 10, 11, -9, -10, -11]]], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (8,) . 첫번쨰 축 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (4,2) . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [78], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.concat은 차원을 증가시키면서 결합해주는것은 아니다. | . tf.stack . - (4) stack (4) =&gt; (4,2) . (4,) stack (4,) =&gt; (4,2) // 두번째 축이 비어있다고 인식 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . - (4) stack (4) =&gt; (2,4) . (,4) stack (,4) =&gt; (2,4) // 첫번째 축이 비어있다고 인식 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,2,3,4,5) . 첫번째축이 비어있다고 인식 | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,2,3,4,5) . 두번째축이 비어있다고 인식 | (2, ,3,4,5) stack (2, ,3,4,5) | (2,2,3,4,5) | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . - (2,3,4,5) stack (2,3,4,5) =&gt; (2,3,4,5,2) . 마지막축이 비어있다고 인식 | (2,3,4,5,) stack (2,3,4,5,) | (2,3,4,5,2) | . a=tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b=-a c=-a*2 . 아래의 코드에서 ??를 어떻게 바꿔야?? . tf.stack([a,b],axis=??) ## 숙제 .",
            "url": "https://guebin.github.io/DS2022/2022/03/16/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2022/03/16/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9416%EC%9D%BC.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(2주차) 3월14일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) # check GPU . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - (2,) vector . lst = [1,2] . lst . [1, 2] . lst[0] # . 1 . lst[1] . 2 . - (2,2) matrix . lst = [[1,2],[3,4]] . lst[0][0] . 1 . lst[1][1] . 4 . 이것은 아래와 같은 매트릭스처럼 생각할 수 있음 . 1 2 3 4 . print(lst[0][0]) # (1,1) print(lst[0][1]) # (1,2) print(lst[1][0]) # (2,1) print(lst[1][1]) # (2,2) . 1 2 3 4 . 매트릭스는 아니지만 매트릭스 같음 . - (4,1) matrix . lst = [[1,2],[3,4],[5,6],[7,8]] lst # (4,2) . [[1, 2], [3, 4], [5, 6], [7, 8]] . lst = [[1],[3],[5],[7]] lst # (4,1) matrix = 길이가 4인 col-vector . [[1], [3], [5], [7]] . - (1,4) matrix . lst = [[1,2,3,4],[5,6,7,8]] lst # (2,4) matrix . [[1, 2, 3, 4], [5, 6, 7, 8]] . lst = [[1,2,3,4]] lst # (1,4) matrix = 길이가 4인 row-vector . [[1, 2, 3, 4]] . - 3차원 . lst = [[[1,2],[3,4]], [[5,6],[7,8]]] lst . [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] . print(lst[0][0][0]) print(lst[0][0][1]) print(lst[0][1][0]) print(lst[0][1][1]) print(lst[1][0][0]) print(lst[1][0][1]) print(lst[1][1][0]) print(lst[1][1][1]) . 1 2 3 4 5 6 7 8 . &#49440;&#50616; . - 스칼라 . _scalar =tf.constant(1) _scalar . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . - 벡터 . _vector=tf.constant([1,2,3]) _vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _vector[-2] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt; . - 매트릭스 . _matrix=tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0,1] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt; . &#53440;&#51077; . type(_scalar) . tensorflow.python.framework.ops.EagerTensor . &#51064;&#45937;&#49905; . _matrix=tf.constant([[1,2,3],[3,4,5]]) _matrix . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [3, 4, 5]], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . _matrix[:,0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt; . _matrix[:,-1] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 5], dtype=int32)&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 불편한점 . dtype이 모든 원소가 똑같아야한다. (그럴수 있음) | 값을 바꿀 수가 없다. | dtype이 다르면 연산이 불가능함. | - 값을 바꿀 수 없다. . _matrix=tf.constant([[1,2,3],[3,4,5]]) _matrix . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [3, 4, 5]], dtype=int32)&gt; . _matrix[0,0]=22 . TypeError Traceback (most recent call last) Input In [138], in &lt;cell line: 1&gt;() -&gt; 1 _matrix[0,0]=22 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - dtype이 다르면 연산이 불가능함 . tf.constant([1.1,2])+tf.constant([3,4]) . InvalidArgumentError Traceback (most recent call last) Input In [139], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+tf.constant([3,4]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] . tf.constant([1.1,2])+tf.constant([3.0,4.0]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.1, 6. ], dtype=float32)&gt; . - 같은 float형이라도 불가능할 때가 있음 . tf.constant([1.1,2]) . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.1, 2. ], dtype=float32)&gt; . tf.constant([3.0,4.0],dtype=tf.float64) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3., 4.])&gt; . tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) . InvalidArgumentError Traceback (most recent call last) Input In [143], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant([1.1,2])+tf.constant([3.0,4.0],dtype=tf.float64) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . _vector = tf.constant([1,2,3,4]) _vector . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . np.array(_vector) # 방법1 . array([1, 2, 3, 4], dtype=int32) . _vector.numpy() . array([1, 2, 3, 4], dtype=int32) . &#50672;&#49328; . - 더하기 . a=tf.constant([1,2,3]) b=tf.constant([4,5,6]) a+b . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)&gt; . - 곱하기 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[4,5],[6,7]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 4, 10], [18, 28]], dtype=int32)&gt; . a,b . (&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[4, 5], [6, 7]], dtype=int32)&gt;) . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 4, 10], [18, 28]], dtype=int32)&gt; . - 행렬곱 . a=tf.constant([[1,0],[0,1]]) b= tf.constant([[1],[22]]) . a @ b # (2,2) matrix @ (2,1) matrix . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[ 1], [22]], dtype=int32)&gt; . b @ a # (2,1) matrix @ (2,2) matrix . InvalidArgumentError Traceback (most recent call last) Input In [154], in &lt;cell line: 1&gt;() -&gt; 1 b @ a File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,1], In[1]: [2,2] [Op:MatMul] . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[ 1], [22]], dtype=int32)&gt; . - 역행렬 . a= tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) Input In [157], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1506, in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a= tf.constant([[1.0,2.0],[3.0,4.0]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 2.], [3., 4.]], dtype=float32)&gt; . tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-2. , 1. ], [ 1.5, -0.5]], dtype=float32)&gt; . a @ tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . - tf.linalg. + tab을 누르면 쓸만한게 조금 나온다. . a=tf.constant([[1.0,2.0],[3.0,4.0]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 2.], [3., 4.]], dtype=float32)&gt; . tf.linalg.det(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . - 단순한 집계함수들 . a=tf.constant([[1,2,3],[4,5,6]]) a . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 2, 3], [4, 5, 6]], dtype=int32)&gt; . tf.reduce_sum(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=21&gt; . tf.reduce_max(a) . &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt; . - 행렬곱 고급 . _I=tf.constant([[1.0,0.0],[0.0,1.0]]) _I . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 0.], [0., 1.]], dtype=float32)&gt; . _x = tf.constant([22.0,33.0]) _x . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([22., 33.], dtype=float32)&gt; . (상황1) . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [169], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . _x @ _I . InvalidArgumentError Traceback (most recent call last) Input In [170], in &lt;cell line: 1&gt;() -&gt; 1 _x @ _I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . (상황2) . _x = tf.constant([[1.0],[2.0]]) _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[1.], [2.]], dtype=float32)&gt; . _I @ _x . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[1.], [2.]], dtype=float32)&gt; . _x @ _I . InvalidArgumentError Traceback (most recent call last) Input In [173], in &lt;cell line: 1&gt;() -&gt; 1 _x @ _I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,1], In[1]: [2,2] [Op:MatMul] . (상황3) . _x = tf.constant([[1.0,2.0]]) _x . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 2.]], dtype=float32)&gt; . _I @ _x . InvalidArgumentError Traceback (most recent call last) Input In [175], in &lt;cell line: 1&gt;() -&gt; 1 _I @ _x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Matrix size-incompatible: In[0]: [2,2], In[1]: [1,2] [Op:MatMul] . _x @ _I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 2.]], dtype=float32)&gt; . (넘파이는 다르다..) . np.array([22,33]) @ np.diag([1,1]) # (2,) @ (2,2) ==&gt; (2,)를 (1,2)벡터로 해석해줘서 곱하기 수행, 수행결과는 다시 (2,)로 저장 . array([22, 33]) . np.array([[22],[33]]) @ np.diag([1,1]) # (2,1) @ (2,2) . ValueError Traceback (most recent call last) Input In [181], in &lt;cell line: 1&gt;() -&gt; 1 np.array([[22],[33]]) @ np.diag([1,1]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . &#54805;&#53468;&#48320;&#54872; . - tf.reshape 기본 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.reshape(a,(4,1)) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . tf.reshape(a,(1,4)) . &lt;tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 2, 3, 4]], dtype=int32)&gt; . - tf.reshape 응용 . a= tf.constant([0,1,2,3,4,5,6,7,8,9,10,11]) # length 12 vector a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . tf.reshape(a,(3,-1)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]], dtype=int32)&gt; . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 대각행렬선언 . tf.constant(np.diag([1,2,3])) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . - 1만 포함한 텐서를 만들고 싶음 . tf.ones([3,4]) . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32)&gt; . tf.reshape(tf.constant([1]*12),(3,4)) . &lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], dtype=int32)&gt; .",
            "url": "https://guebin.github.io/DS2022/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(1주차) 3월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . &#44053;&#51032;&#48372;&#52649;&#51088;&#47308; . - https://github.com/guebin/DS2022/blob/master/_notebooks/2022-03-07-supp1.pdf . - https://github.com/guebin/DS2022/blob/master/_notebooks/2022-03-07-supp2.pdf . &#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: (선형대수학 $ to$) 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 나는 동네에 커피점을 하나 차렸음. | 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. | 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 &#39;온도 -&gt; 아이스아메리카노 판매량 예측&#39; 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) | . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . 온도 ${ bf x}$가 아래와 같다고 하자. . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . [&lt;matplotlib.lines.Line2D at 0x7f114c62d7e0&gt;] . - 우리의 목표: 파란색점 $ to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $ beta_0, beta_1$) 가 존재할것 같다. . $y_{i} approx ax_{i}+b$ | $y_{i} approx beta_1 x_{i}+ beta_0$ | . - 어림짐작으로 $a,b$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $a=2, b=15$ 혹은 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521086&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . - 풀어보자. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.84898&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;) # 초록색선: ture, 세상의법칙 . [&lt;matplotlib.lines.Line2D at 0x7f1128716d70&gt;] . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. . - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . - 모형의 매트릭스화 . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면: $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ &lt;-- 외우세요 . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X=tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945175 , 2.2156773], dtype=float32)&gt; . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573243234018&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.215704607783491&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457324, 2.21570461])&gt; . &#50526;&#51004;&#47196; &#54624;&#44163; . - 선형대수학의 미분이론.. . - 실습 (tensorflow에서 매트릭스를 자유롭게 다루비) .",
            "url": "https://guebin.github.io/DS2022/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "date": " • Mar 7, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널 1 | . 2022년 1학기 종료후 폐쇄예정 &#8617; . |",
          "url": "https://guebin.github.io/DS2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/DS2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}